{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dev Phase 5: Training Pipeline\n",
        "\n",
        "Training notebook for the MIGT-TVDT model with:\n",
        "- Mixed precision training (AMP)\n",
        "- Gradient accumulation\n",
        "- Learning rate scheduling with warmup\n",
        "- Early stopping\n",
        "- Checkpointing\n",
        "\n",
        "**Tests:**\n",
        "1. Loss functions (pinball loss, per-quantile breakdown)\n",
        "2. Learning rate scheduler (warmup, cosine annealing)\n",
        "3. Training step (single batch, gradient flow)\n",
        "4. Validation step (metrics computation)\n",
        "5. Full training loop (mini run)\n",
        "6. Checkpoint save/load\n",
        "7. Early stopping behavior\n",
        "8. Phase 3/4 integration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup: Mount drive, add paths\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0, '/content/drive/MyDrive/Colab Notebooks/Transformers/FP/src')\n",
        "\n",
        "!pip install pyyaml -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import yaml\n",
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "# Training imports\n",
        "from training.loss_functions import PinballLoss, CombinedQuantileLoss\n",
        "from training.scheduler import WarmupCosineScheduler, LinearWarmupScheduler\n",
        "from training.trainer import Trainer, EarlyStopping, create_trainer\n",
        "\n",
        "# Model imports\n",
        "from model.migt_tvdt import MIGT_TVDT\n",
        "\n",
        "# Data imports\n",
        "from data.dataset import NQDataModule, collate_fn\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load configurations\n",
        "BASE_DIR = Path('/content/drive/MyDrive/Colab Notebooks/Transformers/FP')\n",
        "\n",
        "with open(BASE_DIR / 'configs/model_config.yaml') as f:\n",
        "    model_config = yaml.safe_load(f)\n",
        "\n",
        "with open(BASE_DIR / 'configs/training_config.yaml') as f:\n",
        "    train_config = yaml.safe_load(f)\n",
        "\n",
        "print(\"Model config loaded\")\n",
        "print(f\"  d_model: {model_config['model']['d_model']}\")\n",
        "print(f\"  n_variables: {model_config['model']['n_variables']}\")\n",
        "\n",
        "print(\"\\nTraining config loaded\")\n",
        "print(f\"  batch_size: {train_config['training']['batch_size']}\")\n",
        "print(f\"  max_epochs: {train_config['training']['max_epochs']}\")\n",
        "print(f\"  lr: {train_config['optimizer']['lr']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test parameters\n",
        "B = 4  # Batch size for unit tests\n",
        "H = 5  # Horizons\n",
        "Q = 7  # Quantiles\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Testing on device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 1: Loss Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_loss_functions():\n",
        "    \"\"\"Test pinball loss computation and properties.\"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"TEST 1: Loss Functions\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    quantiles = [0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95]\n",
        "    \n",
        "    # 1.1: Basic pinball loss\n",
        "    print(\"\\n1.1 PinballLoss basic computation\")\n",
        "    loss_fn = PinballLoss(quantiles)\n",
        "    \n",
        "    # Perfect predictions (all quantiles equal target)\n",
        "    predictions = torch.zeros(B, H, Q)\n",
        "    targets = torch.zeros(B, H)\n",
        "    \n",
        "    loss = loss_fn(predictions, targets)\n",
        "    print(f\"  Loss with perfect predictions: {loss.item():.6f}\")\n",
        "    assert loss.item() == 0.0, \"Perfect predictions should have zero loss\"\n",
        "    print(\"  [PASS] Zero loss for perfect predictions\")\n",
        "    \n",
        "    # 1.2: Asymmetric penalty check\n",
        "    print(\"\\n1.2 Asymmetric penalty (quantile tau=0.9)\")\n",
        "    \n",
        "    # Underprediction (target > prediction) should be penalized more for high tau\n",
        "    pred_under = torch.zeros(B, H, Q)\n",
        "    target_above = torch.ones(B, H)  # Target above prediction\n",
        "    loss_under = loss_fn(pred_under, target_above)\n",
        "    \n",
        "    pred_over = torch.ones(B, H, Q) * 2\n",
        "    target_below = torch.ones(B, H)  # Target below prediction\n",
        "    loss_over = loss_fn(pred_over, target_below)\n",
        "    \n",
        "    print(f\"  Underprediction loss: {loss_under.item():.6f}\")\n",
        "    print(f\"  Overprediction loss: {loss_over.item():.6f}\")\n",
        "    print(\"  [PASS] Asymmetric losses computed\")\n",
        "    \n",
        "    # 1.3: Per-quantile breakdown\n",
        "    print(\"\\n1.3 Per-quantile loss breakdown\")\n",
        "    predictions = torch.randn(B, H, Q)\n",
        "    targets = torch.randn(B, H)\n",
        "    \n",
        "    q_losses = loss_fn.per_quantile_loss(predictions, targets)\n",
        "    print(f\"  Per-quantile losses: {q_losses}\")\n",
        "    assert len(q_losses) == Q, f\"Expected {Q} quantile losses\"\n",
        "    print(\"  [PASS] Per-quantile breakdown\")\n",
        "    \n",
        "    # 1.4: Per-horizon breakdown\n",
        "    print(\"\\n1.4 Per-horizon loss breakdown\")\n",
        "    h_losses = loss_fn.per_horizon_loss(predictions, targets)\n",
        "    print(f\"  Per-horizon losses: {h_losses}\")\n",
        "    assert len(h_losses) == H, f\"Expected {H} horizon losses\"\n",
        "    print(\"  [PASS] Per-horizon breakdown\")\n",
        "    \n",
        "    # 1.5: Combined loss\n",
        "    print(\"\\n1.5 CombinedQuantileLoss\")\n",
        "    combined_loss = CombinedQuantileLoss(quantiles)\n",
        "    \n",
        "    loss_dict = combined_loss(predictions, targets)\n",
        "    print(f\"  Total loss: {loss_dict['total'].item():.6f}\")\n",
        "    print(f\"  Pinball loss: {loss_dict['pinball'].item():.6f}\")\n",
        "    assert 'total' in loss_dict and 'pinball' in loss_dict and 'crossing' in loss_dict\n",
        "    print(f\"  Crossing loss: {loss_dict['crossing'].item():.6f}\")\n",
        "    print(\"  [PASS] Combined loss components\")\n",
        "    \n",
        "    # 1.6: Metrics computation\n",
        "    print(\"\\n1.6 Metrics computation\")\n",
        "    metrics = combined_loss.get_metrics(predictions, targets)\n",
        "    print(f\"  PICP 80%: {metrics['picp_80']:.3f}\")\n",
        "    print(f\"  Interval 80 mean: {metrics['interval_80_mean']:.4f}\")\n",
        "    assert 'picp_80' in metrics and 'coverage_q50' in metrics\n",
        "    print(\"  [PASS] Metrics computed\")\n",
        "    \n",
        "    # 1.7: Gradient flow\n",
        "    print(\"\\n1.7 Gradient flow through loss\")\n",
        "    predictions = torch.randn(B, H, Q, requires_grad=True)\n",
        "    targets = torch.randn(B, H)\n",
        "    \n",
        "    loss = loss_fn(predictions, targets)\n",
        "    loss.backward()\n",
        "    \n",
        "    assert predictions.grad is not None, \"Gradients not computed\"\n",
        "    assert not torch.isnan(predictions.grad).any(), \"NaN gradients\"\n",
        "    print(f\"  Gradient norm: {predictions.grad.norm().item():.6f}\")\n",
        "    print(\"  [PASS] Gradients flow correctly\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"TEST 1 COMPLETE: All loss function tests passed\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "test_loss_functions()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 2: Learning Rate Scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_scheduler():\n",
        "    \"\"\"Test learning rate scheduling with warmup and cosine annealing.\"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"TEST 2: Learning Rate Scheduler\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Create dummy model and optimizer\n",
        "    model = nn.Linear(10, 10)\n",
        "    base_lr = 1e-4\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=base_lr)\n",
        "    \n",
        "    # 2.1: Warmup phase\n",
        "    print(\"\\n2.1 Warmup phase\")\n",
        "    scheduler = WarmupCosineScheduler(\n",
        "        optimizer,\n",
        "        warmup_steps=100,\n",
        "        t_0=10,\n",
        "        t_mult=2,\n",
        "        eta_min=1e-6\n",
        "    )\n",
        "    \n",
        "    # Simulate warmup\n",
        "    warmup_lrs = []\n",
        "    for step in range(100):\n",
        "        scheduler.step_batch()\n",
        "        warmup_lrs.append(optimizer.param_groups[0]['lr'])\n",
        "    \n",
        "    print(f\"  LR at step 0: {warmup_lrs[0]:.2e}\")\n",
        "    print(f\"  LR at step 50: {warmup_lrs[50]:.2e}\")\n",
        "    print(f\"  LR at step 99: {warmup_lrs[99]:.2e}\")\n",
        "    \n",
        "    # Verify linear increase\n",
        "    assert warmup_lrs[0] < warmup_lrs[50] < warmup_lrs[99], \"LR should increase during warmup\"\n",
        "    assert abs(warmup_lrs[99] - base_lr) < 1e-6, f\"LR should reach base_lr at warmup end\"\n",
        "    print(\"  [PASS] Linear warmup\")\n",
        "    \n",
        "    # 2.2: Cosine annealing\n",
        "    print(\"\\n2.2 Cosine annealing phase\")\n",
        "    \n",
        "    epoch_lrs = []\n",
        "    for epoch in range(30):\n",
        "        scheduler.step()\n",
        "        epoch_lrs.append(optimizer.param_groups[0]['lr'])\n",
        "    \n",
        "    print(f\"  LR at epoch 0: {epoch_lrs[0]:.2e}\")\n",
        "    print(f\"  LR at epoch 9 (end of first cycle): {epoch_lrs[9]:.2e}\")\n",
        "    print(f\"  LR at epoch 10 (restart): {epoch_lrs[10]:.2e}\")\n",
        "    print(f\"  LR at epoch 29: {epoch_lrs[29]:.2e}\")\n",
        "    \n",
        "    # Verify cosine decay\n",
        "    assert epoch_lrs[0] > epoch_lrs[5], \"LR should decrease in first cycle\"\n",
        "    assert epoch_lrs[9] < epoch_lrs[10], \"LR should jump at restart (epoch 10)\"\n",
        "    print(\"  [PASS] Cosine annealing with restarts\")\n",
        "    \n",
        "    # 2.3: State dict save/load\n",
        "    print(\"\\n2.3 State dict save/load\")\n",
        "    state = scheduler.state_dict()\n",
        "    \n",
        "    # Create new scheduler and load state\n",
        "    optimizer2 = torch.optim.AdamW(model.parameters(), lr=base_lr)\n",
        "    scheduler2 = WarmupCosineScheduler(\n",
        "        optimizer2, warmup_steps=100, t_0=10, t_mult=2, eta_min=1e-6\n",
        "    )\n",
        "    scheduler2.load_state_dict(state)\n",
        "    \n",
        "    assert scheduler2.warmup_finished == scheduler.warmup_finished\n",
        "    assert scheduler2.epoch_in_cycle == scheduler.epoch_in_cycle\n",
        "    print(\"  [PASS] State dict save/load\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"TEST 2 COMPLETE: All scheduler tests passed\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "test_scheduler()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 3: Training Step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_training_step():\n",
        "    \"\"\"Test single training step with gradient flow.\"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"TEST 3: Training Step\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Create model\n",
        "    model = MIGT_TVDT(model_config['model']).to(device)\n",
        "    model.train()\n",
        "    \n",
        "    # Create optimizer\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=train_config['optimizer']['lr'],\n",
        "        weight_decay=train_config['optimizer']['weight_decay']\n",
        "    )\n",
        "    \n",
        "    # Loss function\n",
        "    loss_fn = CombinedQuantileLoss(\n",
        "        quantiles=train_config['quantile_regression']['quantiles']\n",
        "    )\n",
        "    \n",
        "    # Create synthetic batch\n",
        "    T, V = 288, model_config['model']['n_variables']\n",
        "    batch = {\n",
        "        'features': torch.randn(B, T, V, device=device),\n",
        "        'attention_mask': torch.ones(B, T, dtype=torch.bool, device=device),\n",
        "        'targets': torch.randn(B, H, device=device),\n",
        "        'bar_in_day': torch.arange(T).unsqueeze(0).expand(B, -1).to(device),\n",
        "        'day_of_week': torch.randint(0, 5, (B,), device=device),\n",
        "        'day_of_month': torch.randint(1, 32, (B,), device=device),\n",
        "        'day_of_year': torch.randint(1, 366, (B,), device=device)\n",
        "    }\n",
        "    \n",
        "    # 3.1: Forward pass\n",
        "    print(\"\\n3.1 Forward pass\")\n",
        "    temporal_info = {\n",
        "        'bar_in_day': batch['bar_in_day'],\n",
        "        'day_of_week': batch['day_of_week'],\n",
        "        'day_of_month': batch['day_of_month'],\n",
        "        'day_of_year': batch['day_of_year']\n",
        "    }\n",
        "    \n",
        "    outputs = model(\n",
        "        features=batch['features'],\n",
        "        attention_mask=batch['attention_mask'],\n",
        "        temporal_info=temporal_info\n",
        "    )\n",
        "    \n",
        "    print(f\"  Output shape: {outputs['quantiles'].shape}\")\n",
        "    assert outputs['quantiles'].shape == (B, H, Q), f\"Expected ({B}, {H}, {Q})\"\n",
        "    print(\"  [PASS] Forward pass shape\")\n",
        "    \n",
        "    # 3.2: Loss computation\n",
        "    print(\"\\n3.2 Loss computation\")\n",
        "    loss_dict = loss_fn(outputs['quantiles'], batch['targets'])\n",
        "    loss = loss_dict['total']\n",
        "    \n",
        "    print(f\"  Loss value: {loss.item():.6f}\")\n",
        "    assert not torch.isnan(loss), \"Loss should not be NaN\"\n",
        "    assert loss.item() > 0, \"Loss should be positive\"\n",
        "    print(\"  [PASS] Loss computed\")\n",
        "    \n",
        "    # 3.3: Backward pass\n",
        "    print(\"\\n3.3 Backward pass\")\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    \n",
        "    # Check gradients exist\n",
        "    grad_norms = []\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.grad is not None:\n",
        "            grad_norms.append((name, param.grad.norm().item()))\n",
        "    \n",
        "    print(f\"  Parameters with gradients: {len(grad_norms)}\")\n",
        "    print(f\"  Sample gradient norms:\")\n",
        "    for name, norm in grad_norms[:3]:\n",
        "        print(f\"    {name}: {norm:.6f}\")\n",
        "    \n",
        "    assert len(grad_norms) > 0, \"No gradients computed\"\n",
        "    print(\"  [PASS] Gradients computed\")\n",
        "    \n",
        "    # 3.4: Optimizer step\n",
        "    print(\"\\n3.4 Optimizer step\")\n",
        "    \n",
        "    # Get initial weights\n",
        "    initial_weight = model.output_pool[0].weight.clone()\n",
        "    \n",
        "    optimizer.step()\n",
        "    \n",
        "    # Check weights changed\n",
        "    weight_diff = (model.output_pool[0].weight - initial_weight).abs().mean()\n",
        "    print(f\"  Weight change (mean abs): {weight_diff.item():.8f}\")\n",
        "    assert weight_diff > 0, \"Weights should change after optimizer step\"\n",
        "    print(\"  [PASS] Optimizer step updated weights\")\n",
        "    \n",
        "    # 3.5: Mixed precision\n",
        "    print(\"\\n3.5 Mixed precision training\")\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    with torch.cuda.amp.autocast():\n",
        "        outputs = model(\n",
        "            features=batch['features'],\n",
        "            attention_mask=batch['attention_mask'],\n",
        "            temporal_info=temporal_info\n",
        "        )\n",
        "        loss_dict = loss_fn(outputs['quantiles'], batch['targets'])\n",
        "        loss = loss_dict['total']\n",
        "    \n",
        "    scaler.scale(loss).backward()\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "    \n",
        "    print(f\"  AMP loss: {loss.item():.6f}\")\n",
        "    print(f\"  Scaler scale: {scaler.get_scale():.1f}\")\n",
        "    print(\"  [PASS] Mixed precision training\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"TEST 3 COMPLETE: All training step tests passed\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "test_training_step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 4: Validation Step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_validation_step():\n",
        "    \"\"\"Test validation with metrics computation.\"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"TEST 4: Validation Step\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    model = MIGT_TVDT(model_config['model']).to(device)\n",
        "    model.eval()\n",
        "    \n",
        "    loss_fn = CombinedQuantileLoss(\n",
        "        quantiles=train_config['quantile_regression']['quantiles']\n",
        "    )\n",
        "    \n",
        "    # Create synthetic validation batch\n",
        "    T, V = 288, model_config['model']['n_variables']\n",
        "    batch = {\n",
        "        'features': torch.randn(B, T, V, device=device),\n",
        "        'attention_mask': torch.ones(B, T, dtype=torch.bool, device=device),\n",
        "        'targets': torch.randn(B, H, device=device),\n",
        "        'bar_in_day': torch.arange(T).unsqueeze(0).expand(B, -1).to(device),\n",
        "        'day_of_week': torch.randint(0, 5, (B,), device=device),\n",
        "        'day_of_month': torch.randint(1, 32, (B,), device=device),\n",
        "        'day_of_year': torch.randint(1, 366, (B,), device=device)\n",
        "    }\n",
        "    \n",
        "    # 4.1: Validation forward pass (no gradients)\n",
        "    print(\"\\n4.1 Validation forward pass\")\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        temporal_info = {\n",
        "            'bar_in_day': batch['bar_in_day'],\n",
        "            'day_of_week': batch['day_of_week'],\n",
        "            'day_of_month': batch['day_of_month'],\n",
        "            'day_of_year': batch['day_of_year']\n",
        "        }\n",
        "        \n",
        "        outputs = model(\n",
        "            features=batch['features'],\n",
        "            attention_mask=batch['attention_mask'],\n",
        "            temporal_info=temporal_info\n",
        "        )\n",
        "        \n",
        "        loss_dict = loss_fn(outputs['quantiles'], batch['targets'])\n",
        "    \n",
        "    print(f\"  Val loss: {loss_dict['total'].item():.6f}\")\n",
        "    print(\"  [PASS] No-gradient validation\")\n",
        "    \n",
        "    # 4.2: Metrics computation\n",
        "    print(\"\\n4.2 Detailed metrics\")\n",
        "    \n",
        "    predictions = outputs['quantiles'].cpu()\n",
        "    targets = batch['targets'].cpu()\n",
        "    \n",
        "    metrics = loss_fn.get_metrics(predictions, targets)\n",
        "    \n",
        "    print(f\"  PICP 80%: {metrics['picp_80']:.3f}\")\n",
        "    print(f\"  Coverage q50: {metrics['coverage_q50']:.3f}\")\n",
        "    print(f\"  Interval 80 mean: {metrics['interval_80_mean']:.4f}\")\n",
        "    print(f\"  Loss 15m: {metrics['loss_15m']:.6f}\")\n",
        "    \n",
        "    # Check all expected metrics present\n",
        "    expected_keys = ['picp_80', 'coverage_q50', 'interval_80_mean', 'loss_15m']\n",
        "    for key in expected_keys:\n",
        "        assert key in metrics, f\"Missing metric: {key}\"\n",
        "    print(\"  [PASS] All metrics computed\")\n",
        "    \n",
        "    # 4.3: Non-crossing verification\n",
        "    print(\"\\n4.3 Quantile non-crossing verification\")\n",
        "    \n",
        "    # Check all quantiles are monotonically increasing\n",
        "    diffs = predictions[:, :, 1:] - predictions[:, :, :-1]\n",
        "    all_positive = (diffs >= 0).all()\n",
        "    \n",
        "    print(f\"  Min quantile diff: {diffs.min().item():.6f}\")\n",
        "    print(f\"  All non-crossing: {all_positive.item()}\")\n",
        "    assert all_positive, \"Quantiles should be non-crossing\"\n",
        "    print(\"  [PASS] Non-crossing quantiles verified\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"TEST 4 COMPLETE: All validation tests passed\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "test_validation_step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 5: Full Training Loop (Mini)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_training_loop_mini():\n",
        "    \"\"\"Test full training loop with synthetic data.\"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"TEST 5: Full Training Loop (Mini)\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Create synthetic dataset\n",
        "    from torch.utils.data import TensorDataset, DataLoader\n",
        "    \n",
        "    T, V = 288, model_config['model']['n_variables']\n",
        "    n_samples = 32\n",
        "    \n",
        "    features = torch.randn(n_samples, T, V)\n",
        "    masks = torch.ones(n_samples, T, dtype=torch.bool)\n",
        "    targets = torch.randn(n_samples, H)\n",
        "    bar_in_day = torch.arange(T).unsqueeze(0).expand(n_samples, -1)\n",
        "    day_of_week = torch.randint(0, 5, (n_samples,))\n",
        "    day_of_month = torch.randint(1, 32, (n_samples,))\n",
        "    day_of_year = torch.randint(1, 366, (n_samples,))\n",
        "    \n",
        "    # Custom collate for synthetic data\n",
        "    def synthetic_collate(batch):\n",
        "        indices = torch.tensor(batch)\n",
        "        return {\n",
        "            'features': features[indices],\n",
        "            'attention_mask': masks[indices],\n",
        "            'targets': targets[indices],\n",
        "            'bar_in_day': bar_in_day[indices],\n",
        "            'day_of_week': day_of_week[indices],\n",
        "            'day_of_month': day_of_month[indices],\n",
        "            'day_of_year': day_of_year[indices]\n",
        "        }\n",
        "    \n",
        "    train_loader = DataLoader(\n",
        "        list(range(n_samples // 2)),\n",
        "        batch_size=4,\n",
        "        shuffle=True,\n",
        "        collate_fn=synthetic_collate\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        list(range(n_samples // 2, n_samples)),\n",
        "        batch_size=4,\n",
        "        collate_fn=synthetic_collate\n",
        "    )\n",
        "    \n",
        "    # Create model and trainer config\n",
        "    model = MIGT_TVDT(model_config['model']).to(device)\n",
        "    \n",
        "    mini_config = {\n",
        "        'training': {\n",
        "            'batch_size': 4,\n",
        "            'gradient_accumulation_steps': 1,\n",
        "            'max_epochs': 3,\n",
        "            'early_stopping_patience': 5,\n",
        "            'mixed_precision': True\n",
        "        },\n",
        "        'optimizer': {\n",
        "            'lr': 1e-4,\n",
        "            'weight_decay': 0.01,\n",
        "            'betas': [0.9, 0.999]\n",
        "        },\n",
        "        'scheduler': {\n",
        "            'warmup_steps': 10,\n",
        "            't_0': 2,\n",
        "            't_mult': 2,\n",
        "            'eta_min': 1e-6\n",
        "        },\n",
        "        'regularization': {\n",
        "            'gradient_clip_norm': 1.0\n",
        "        },\n",
        "        'quantile_regression': {\n",
        "            'quantiles': [0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95],\n",
        "            'crossing_weight': 0.0\n",
        "        },\n",
        "        'checkpointing': {\n",
        "            'save_top_k': 2\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    output_dir = Path('/content/test_outputs')\n",
        "    output_dir.mkdir(exist_ok=True)\n",
        "    \n",
        "    # 5.1: Create trainer\n",
        "    print(\"\\n5.1 Create trainer\")\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        config=mini_config,\n",
        "        train_loader=train_loader,\n",
        "        val_loader=val_loader,\n",
        "        output_dir=output_dir,\n",
        "        device=device\n",
        "    )\n",
        "    print(\"  [PASS] Trainer created\")\n",
        "    \n",
        "    # 5.2: Run training\n",
        "    print(\"\\n5.2 Run training (3 epochs)\")\n",
        "    history = trainer.train()\n",
        "    \n",
        "    print(f\"\\n  Final train loss: {history['train_loss'][-1]:.6f}\")\n",
        "    print(f\"  Final val loss: {history['val_loss'][-1]:.6f}\")\n",
        "    print(f\"  Best val loss: {trainer.best_val_loss:.6f}\")\n",
        "    \n",
        "    assert len(history['train_loss']) == 3, \"Should have 3 epochs\"\n",
        "    assert len(history['val_loss']) == 3\n",
        "    print(\"  [PASS] Training completed\")\n",
        "    \n",
        "    # 5.3: Verify loss decreased (with high probability)\n",
        "    print(\"\\n5.3 Loss trend\")\n",
        "    # Note: With random data, loss may not always decrease\n",
        "    # We just verify no NaN/inf\n",
        "    for loss in history['train_loss']:\n",
        "        assert np.isfinite(loss), \"Loss should be finite\"\n",
        "    print(f\"  Train losses: {history['train_loss']}\")\n",
        "    print(\"  [PASS] All losses finite\")\n",
        "    \n",
        "    # 5.4: Check files created\n",
        "    print(\"\\n5.4 Output files\")\n",
        "    assert (output_dir / 'checkpoint_latest.pt').exists()\n",
        "    assert (output_dir / 'checkpoint_best.pt').exists()\n",
        "    assert (output_dir / 'training_history.json').exists()\n",
        "    print(\"  checkpoint_latest.pt: exists\")\n",
        "    print(\"  checkpoint_best.pt: exists\")\n",
        "    print(\"  training_history.json: exists\")\n",
        "    print(\"  [PASS] All output files created\")\n",
        "    \n",
        "    # Cleanup\n",
        "    import shutil\n",
        "    shutil.rmtree(output_dir)\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"TEST 5 COMPLETE: Full training loop test passed\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "test_training_loop_mini()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 6: Checkpoint Save/Load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_checkpoint_save_load():\n",
        "    \"\"\"Test checkpoint saving and loading.\"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"TEST 6: Checkpoint Save/Load\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    output_dir = Path('/content/test_checkpoint')\n",
        "    output_dir.mkdir(exist_ok=True)\n",
        "    \n",
        "    # Create and initialize model\n",
        "    model = MIGT_TVDT(model_config['model']).to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
        "    scheduler = WarmupCosineScheduler(optimizer, warmup_steps=100)\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "    \n",
        "    # Simulate some training\n",
        "    for _ in range(50):\n",
        "        scheduler.step_batch()\n",
        "    \n",
        "    # 6.1: Save checkpoint\n",
        "    print(\"\\n6.1 Save checkpoint\")\n",
        "    checkpoint = {\n",
        "        'epoch': 5,\n",
        "        'global_step': 500,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict': scheduler.state_dict(),\n",
        "        'scaler_state_dict': scaler.state_dict(),\n",
        "        'val_loss': 0.123,\n",
        "        'best_val_loss': 0.100,\n",
        "        'config': model_config\n",
        "    }\n",
        "    \n",
        "    checkpoint_path = output_dir / 'test_checkpoint.pt'\n",
        "    torch.save(checkpoint, checkpoint_path)\n",
        "    print(f\"  Saved to: {checkpoint_path}\")\n",
        "    print(f\"  File size: {checkpoint_path.stat().st_size / 1e6:.2f} MB\")\n",
        "    print(\"  [PASS] Checkpoint saved\")\n",
        "    \n",
        "    # 6.2: Load checkpoint into new model\n",
        "    print(\"\\n6.2 Load checkpoint\")\n",
        "    \n",
        "    model2 = MIGT_TVDT(model_config['model']).to(device)\n",
        "    optimizer2 = torch.optim.AdamW(model2.parameters(), lr=1e-4)\n",
        "    scheduler2 = WarmupCosineScheduler(optimizer2, warmup_steps=100)\n",
        "    scaler2 = torch.cuda.amp.GradScaler()\n",
        "    \n",
        "    loaded = torch.load(checkpoint_path, map_location=device)\n",
        "    \n",
        "    model2.load_state_dict(loaded['model_state_dict'])\n",
        "    optimizer2.load_state_dict(loaded['optimizer_state_dict'])\n",
        "    scheduler2.load_state_dict(loaded['scheduler_state_dict'])\n",
        "    scaler2.load_state_dict(loaded['scaler_state_dict'])\n",
        "    \n",
        "    print(f\"  Loaded epoch: {loaded['epoch']}\")\n",
        "    print(f\"  Loaded global_step: {loaded['global_step']}\")\n",
        "    print(f\"  Loaded val_loss: {loaded['val_loss']}\")\n",
        "    print(\"  [PASS] Checkpoint loaded\")\n",
        "    \n",
        "    # 6.3: Verify model outputs match\n",
        "    print(\"\\n6.3 Verify model outputs match\")\n",
        "    \n",
        "    model.eval()\n",
        "    model2.eval()\n",
        "    \n",
        "    T, V = 288, model_config['model']['n_variables']\n",
        "    test_input = torch.randn(1, T, V, device=device)\n",
        "    test_mask = torch.ones(1, T, dtype=torch.bool, device=device)\n",
        "    temporal_info = {\n",
        "        'bar_in_day': torch.arange(T).unsqueeze(0).to(device),\n",
        "        'day_of_week': torch.tensor([0], device=device),\n",
        "        'day_of_month': torch.tensor([1], device=device),\n",
        "        'day_of_year': torch.tensor([1], device=device)\n",
        "    }\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        out1 = model(test_input, test_mask, temporal_info)['quantiles']\n",
        "        out2 = model2(test_input, test_mask, temporal_info)['quantiles']\n",
        "    \n",
        "    diff = (out1 - out2).abs().max().item()\n",
        "    print(f\"  Max output difference: {diff:.10f}\")\n",
        "    assert diff < 1e-6, f\"Outputs should match, got diff={diff}\"\n",
        "    print(\"  [PASS] Model outputs match\")\n",
        "    \n",
        "    # Cleanup\n",
        "    import shutil\n",
        "    shutil.rmtree(output_dir)\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"TEST 6 COMPLETE: Checkpoint save/load test passed\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "test_checkpoint_save_load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 7: Early Stopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_early_stopping():\n",
        "    \"\"\"Test early stopping behavior.\"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"TEST 7: Early Stopping\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # 7.1: Basic early stopping\n",
        "    print(\"\\n7.1 Basic early stopping (patience=3)\")\n",
        "    \n",
        "    es = EarlyStopping(patience=3, min_delta=0.001, mode='min')\n",
        "    \n",
        "    # Simulate improving then stagnating\n",
        "    scores = [1.0, 0.9, 0.8, 0.8, 0.8, 0.8]  # Stagnates after epoch 2\n",
        "    \n",
        "    for i, score in enumerate(scores):\n",
        "        should_stop = es(score)\n",
        "        print(f\"  Epoch {i}: score={score:.2f}, counter={es.counter}, stop={should_stop}\")\n",
        "        \n",
        "        if should_stop:\n",
        "            print(f\"  Stopped at epoch {i}\")\n",
        "            break\n",
        "    \n",
        "    assert should_stop, \"Should have triggered early stopping\"\n",
        "    print(\"  [PASS] Early stopping triggered\")\n",
        "    \n",
        "    # 7.2: No stopping with improvement\n",
        "    print(\"\\n7.2 No stopping with improvement\")\n",
        "    \n",
        "    es2 = EarlyStopping(patience=3, min_delta=0.001, mode='min')\n",
        "    scores2 = [1.0, 0.9, 0.8, 0.7, 0.6, 0.5]\n",
        "    \n",
        "    for i, score in enumerate(scores2):\n",
        "        should_stop = es2(score)\n",
        "        if should_stop:\n",
        "            print(f\"  Unexpectedly stopped at epoch {i}\")\n",
        "            break\n",
        "    \n",
        "    assert not should_stop, \"Should not have stopped\"\n",
        "    print(f\"  Final best score: {es2.best_score:.2f}\")\n",
        "    print(\"  [PASS] No early stopping with improvement\")\n",
        "    \n",
        "    # 7.3: Max mode (for accuracy-like metrics)\n",
        "    print(\"\\n7.3 Max mode (accuracy-like)\")\n",
        "    \n",
        "    es3 = EarlyStopping(patience=2, min_delta=0.01, mode='max')\n",
        "    scores3 = [0.5, 0.6, 0.65, 0.65, 0.65]\n",
        "    \n",
        "    for i, score in enumerate(scores3):\n",
        "        should_stop = es3(score)\n",
        "        if should_stop:\n",
        "            break\n",
        "    \n",
        "    assert should_stop, \"Should stop in max mode\"\n",
        "    print(f\"  Best score: {es3.best_score:.2f}\")\n",
        "    print(\"  [PASS] Max mode early stopping\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"TEST 7 COMPLETE: Early stopping tests passed\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "test_early_stopping()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 8: Phase 3/4 Integration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_phase_integration():\n",
        "    \"\"\"Test integration with Phase 3 data pipeline and Phase 4 model.\"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"TEST 8: Phase 3/4 Integration\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Check for processed data\n",
        "    data_path = BASE_DIR / 'data/processed/nq_features_full.parquet'\n",
        "    \n",
        "    if not data_path.exists():\n",
        "        print(f\"\\n  [SKIP] Data file not found: {data_path}\")\n",
        "        print(\"  Run Phase 3 preprocessing first.\")\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"TEST 8 SKIPPED: Data not available\")\n",
        "        print(\"=\" * 60)\n",
        "        return\n",
        "    \n",
        "    # 8.1: Load data module\n",
        "    print(\"\\n8.1 Load NQDataModule\")\n",
        "    \n",
        "    data_module = NQDataModule(\n",
        "        data_path=data_path,\n",
        "        batch_size=4,  # Small batch for testing\n",
        "        num_workers=0,\n",
        "        pin_memory=False\n",
        "    )\n",
        "    data_module.setup()\n",
        "    \n",
        "    print(f\"  Train samples: {len(data_module.train_dataset):,}\")\n",
        "    print(f\"  Val samples: {len(data_module.val_dataset):,}\")\n",
        "    print(f\"  Test samples: {len(data_module.test_dataset):,}\")\n",
        "    print(\"  [PASS] Data module loaded\")\n",
        "    \n",
        "    # 8.2: Get batch from dataloader\n",
        "    print(\"\\n8.2 Get batch from dataloader\")\n",
        "    \n",
        "    train_loader = data_module.train_dataloader()\n",
        "    batch = next(iter(train_loader))\n",
        "    \n",
        "    print(f\"  features shape: {batch['features'].shape}\")\n",
        "    print(f\"  attention_mask shape: {batch['attention_mask'].shape}\")\n",
        "    print(f\"  targets shape: {batch['targets'].shape}\")\n",
        "    print(f\"  bar_in_day shape: {batch['bar_in_day'].shape}\")\n",
        "    print(\"  [PASS] Batch retrieved\")\n",
        "    \n",
        "    # 8.3: Forward pass through model\n",
        "    print(\"\\n8.3 Forward pass with real data\")\n",
        "    \n",
        "    model = MIGT_TVDT(model_config['model']).to(device)\n",
        "    model.eval()\n",
        "    \n",
        "    # Move batch to device\n",
        "    batch_device = {\n",
        "        k: v.to(device) if isinstance(v, torch.Tensor) else v\n",
        "        for k, v in batch.items()\n",
        "    }\n",
        "    \n",
        "    temporal_info = {\n",
        "        'bar_in_day': batch_device['bar_in_day'],\n",
        "        'day_of_week': batch_device['day_of_week'],\n",
        "        'day_of_month': batch_device['day_of_month'],\n",
        "        'day_of_year': batch_device['day_of_year']\n",
        "    }\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model(\n",
        "            features=batch_device['features'],\n",
        "            attention_mask=batch_device['attention_mask'],\n",
        "            temporal_info=temporal_info\n",
        "        )\n",
        "    \n",
        "    print(f\"  Output shape: {outputs['quantiles'].shape}\")\n",
        "    print(f\"  Output sample (first horizon):\\n{outputs['quantiles'][0, 0]}\")\n",
        "    print(\"  [PASS] Forward pass successful\")\n",
        "    \n",
        "    # 8.4: Loss computation with real data\n",
        "    print(\"\\n8.4 Loss computation with real data\")\n",
        "    \n",
        "    loss_fn = CombinedQuantileLoss(\n",
        "        quantiles=train_config['quantile_regression']['quantiles']\n",
        "    )\n",
        "    \n",
        "    loss_dict = loss_fn(outputs['quantiles'], batch_device['targets'])\n",
        "    \n",
        "    print(f\"  Total loss: {loss_dict['total'].item():.6f}\")\n",
        "    print(f\"  Pinball loss: {loss_dict['pinball'].item():.6f}\")\n",
        "    \n",
        "    assert loss_dict['total'].item() > 0, \"Loss should be positive\"\n",
        "    assert np.isfinite(loss_dict['total'].item()), \"Loss should be finite\"\n",
        "    print(\"  [PASS] Loss computed successfully\")\n",
        "    \n",
        "    # 8.5: Metrics with real data\n",
        "    print(\"\\n8.5 Metrics with real data\")\n",
        "    \n",
        "    metrics = loss_fn.get_metrics(\n",
        "        outputs['quantiles'].cpu(),\n",
        "        batch_device['targets'].cpu()\n",
        "    )\n",
        "    \n",
        "    print(f\"  PICP 80%: {metrics['picp_80']:.3f}\")\n",
        "    print(f\"  Coverage q50: {metrics['coverage_q50']:.3f}\")\n",
        "    print(f\"  Interval 80 mean: {metrics['interval_80_mean']:.6f}\")\n",
        "    print(\"  [PASS] Metrics computed\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"TEST 8 COMPLETE: Phase integration tests passed\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "test_phase_integration()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"DEV PHASE 5: TRAINING PIPELINE TESTS COMPLETE\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\\nAll tests passed. Training pipeline is ready for full training.\")\n",
        "print(\"\\nDelivered components:\")\n",
        "print(\"  - src/training/loss_functions.py\")\n",
        "print(\"  - src/training/scheduler.py\")\n",
        "print(\"  - src/training/trainer.py\")\n",
        "print(\"  - src/training/__init__.py\")\n",
        "print(\"  - configs/training_config.yaml\")\n",
        "print(\"\\nNext steps for full training:\")\n",
        "print(\"  1. Ensure Phase 3 data is preprocessed\")\n",
        "print(\"  2. Copy data to VM: /content/data/\")\n",
        "print(\"  3. Run full training with Trainer class\")\n",
        "print(\"  4. Monitor with WandB or TensorBoard\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}