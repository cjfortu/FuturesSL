{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ht7GzowA64Y"
      },
      "source": [
        "# Dev Phase 5: Training Pipeline\n",
        "\n",
        "Training notebook for the MIGT-TVDT model with:\n",
        "- Mixed precision training (AMP)\n",
        "- Gradient accumulation\n",
        "- Learning rate scheduling with warmup\n",
        "- Early stopping\n",
        "- Checkpointing\n",
        "\n",
        "**Tests:**\n",
        "1. Loss functions (pinball loss, per-quantile breakdown)\n",
        "2. Learning rate scheduler (warmup, cosine annealing)\n",
        "3. Training step (single batch, gradient flow)\n",
        "4. Validation step (metrics computation)\n",
        "5. Full training loop (mini run)\n",
        "6. Checkpoint save/load\n",
        "7. Early stopping behavior\n",
        "8. Phase 3/4 integration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V91kYvuAA64b",
        "outputId": "c7e3f890-7c1b-46fa-fade-27f6a3f92c79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Setup: Mount drive, add paths\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0, '/content/drive/MyDrive/Colab Notebooks/Transformers/FP/src')\n",
        "\n",
        "!pip install pyyaml -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wrqh9YTA64d",
        "outputId": "a438c0a2-f9bc-4b09-e8af-d2f4fc7a8aaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.9.0+cu126\n",
            "CUDA available: True\n",
            "GPU: NVIDIA A100-SXM4-80GB\n",
            "VRAM: 85.2 GB\n"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import yaml\n",
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "# Training imports\n",
        "from training.loss_functions import PinballLoss, CombinedQuantileLoss\n",
        "from training.scheduler import WarmupCosineScheduler, LinearWarmupScheduler\n",
        "from training.trainer import Trainer, EarlyStopping, create_trainer\n",
        "\n",
        "# Model imports\n",
        "from model.migt_tvdt import MIGT_TVDT\n",
        "\n",
        "# Data imports\n",
        "from data.dataset import NQDataModule, collate_fn\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WEVNqlhCA64d",
        "outputId": "ffb73199-86fa-402c-dc31-b72646aaa140"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model config loaded\n",
            "  d_model: 256\n",
            "  n_variables: 24\n",
            "\n",
            "Training config loaded\n",
            "  batch_size: 192\n",
            "  max_epochs: 2\n",
            "  lr: 0.0001\n"
          ]
        }
      ],
      "source": [
        "# Load configurations\n",
        "BASE_DIR = Path('/content/drive/MyDrive/Colab Notebooks/Transformers/FP')\n",
        "\n",
        "with open(BASE_DIR / 'configs/model_config.yaml') as f:\n",
        "    model_config = yaml.safe_load(f)\n",
        "\n",
        "with open(BASE_DIR / 'configs/training_config.yaml') as f:\n",
        "    train_config = yaml.safe_load(f)\n",
        "\n",
        "print(\"Model config loaded\")\n",
        "print(f\"  d_model: {model_config['model']['d_model']}\")\n",
        "print(f\"  n_variables: {model_config['model']['n_variables']}\")\n",
        "\n",
        "print(\"\\nTraining config loaded\")\n",
        "print(f\"  batch_size: {train_config['training']['batch_size']}\")\n",
        "print(f\"  max_epochs: {train_config['training']['max_epochs']}\")\n",
        "print(f\"  lr: {train_config['optimizer']['lr']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2w42kq1A64e",
        "outputId": "b845c76e-82e4-4123-cbc1-6c3dc7034afc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing on device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Test parameters\n",
        "B = 4  # Batch size for unit tests\n",
        "H = 5  # Horizons\n",
        "Q = 7  # Quantiles\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Testing on device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feIeJGXTA64f"
      },
      "source": [
        "## Test 1: Loss Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-YVNrAF3A64g",
        "outputId": "0548b3ff-9205-45b3-9b96-6e55feb174c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "TEST 1: Loss Functions\n",
            "============================================================\n",
            "\n",
            "1.1 PinballLoss basic computation\n",
            "  Loss with perfect predictions: 0.000000\n",
            "  [PASS] Zero loss for perfect predictions\n",
            "\n",
            "1.2 Asymmetric penalty (quantile tau=0.9)\n",
            "  Underprediction loss: 0.500000\n",
            "  Overprediction loss: 0.500000\n",
            "  [PASS] Asymmetric losses computed\n",
            "\n",
            "1.3 Per-quantile loss breakdown\n",
            "  Per-quantile losses: {'q05': 0.2595563232898712, 'q10': 0.433596670627594, 'q25': 0.519175112247467, 'q50': 0.6139335632324219, 'q75': 0.7514042854309082, 'q89': 0.8283659815788269, 'q94': 0.7418087124824524}\n",
            "  [PASS] Per-quantile breakdown\n",
            "\n",
            "1.4 Per-horizon loss breakdown\n",
            "  Per-horizon losses: {'15m': 0.331327885389328, '30m': 0.7983009219169617, '60m': 0.6671174764633179, '2h': 0.6198604702949524, '4h': 0.5461364984512329}\n",
            "  [PASS] Per-horizon breakdown\n",
            "\n",
            "1.5 CombinedQuantileLoss\n",
            "  Total loss: 0.592549\n",
            "  Pinball loss: 0.592549\n",
            "  Crossing loss: 0.000000\n",
            "  [PASS] Combined loss components\n",
            "\n",
            "1.6 Metrics computation\n",
            "  PICP 80%: 0.300\n",
            "  Interval 80 mean: -0.0649\n",
            "  [PASS] Metrics computed\n",
            "\n",
            "1.7 Gradient flow through loss\n",
            "  Gradient norm: 0.052148\n",
            "  [PASS] Gradients flow correctly\n",
            "\n",
            "============================================================\n",
            "TEST 1 COMPLETE: All loss function tests passed\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "def test_loss_functions():\n",
        "    \"\"\"Test pinball loss computation and properties.\"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"TEST 1: Loss Functions\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    quantiles = [0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95]\n",
        "\n",
        "    # 1.1: Basic pinball loss\n",
        "    print(\"\\n1.1 PinballLoss basic computation\")\n",
        "    loss_fn = PinballLoss(quantiles)\n",
        "\n",
        "    # Perfect predictions (all quantiles equal target)\n",
        "    predictions = torch.zeros(B, H, Q)\n",
        "    targets = torch.zeros(B, H)\n",
        "\n",
        "    loss = loss_fn(predictions, targets)\n",
        "    print(f\"  Loss with perfect predictions: {loss.item():.6f}\")\n",
        "    assert loss.item() == 0.0, \"Perfect predictions should have zero loss\"\n",
        "    print(\"  [PASS] Zero loss for perfect predictions\")\n",
        "\n",
        "    # 1.2: Asymmetric penalty check\n",
        "    print(\"\\n1.2 Asymmetric penalty (quantile tau=0.9)\")\n",
        "\n",
        "    # Underprediction (target > prediction) should be penalized more for high tau\n",
        "    pred_under = torch.zeros(B, H, Q)\n",
        "    target_above = torch.ones(B, H)  # Target above prediction\n",
        "    loss_under = loss_fn(pred_under, target_above)\n",
        "\n",
        "    pred_over = torch.ones(B, H, Q) * 2\n",
        "    target_below = torch.ones(B, H)  # Target below prediction\n",
        "    loss_over = loss_fn(pred_over, target_below)\n",
        "\n",
        "    print(f\"  Underprediction loss: {loss_under.item():.6f}\")\n",
        "    print(f\"  Overprediction loss: {loss_over.item():.6f}\")\n",
        "    print(\"  [PASS] Asymmetric losses computed\")\n",
        "\n",
        "    # 1.3: Per-quantile breakdown\n",
        "    print(\"\\n1.3 Per-quantile loss breakdown\")\n",
        "    predictions = torch.randn(B, H, Q)\n",
        "    targets = torch.randn(B, H)\n",
        "\n",
        "    q_losses = loss_fn.per_quantile_loss(predictions, targets)\n",
        "    print(f\"  Per-quantile losses: {q_losses}\")\n",
        "    assert len(q_losses) == Q, f\"Expected {Q} quantile losses\"\n",
        "    print(\"  [PASS] Per-quantile breakdown\")\n",
        "\n",
        "    # 1.4: Per-horizon breakdown\n",
        "    print(\"\\n1.4 Per-horizon loss breakdown\")\n",
        "    h_losses = loss_fn.per_horizon_loss(predictions, targets)\n",
        "    print(f\"  Per-horizon losses: {h_losses}\")\n",
        "    assert len(h_losses) == H, f\"Expected {H} horizon losses\"\n",
        "    print(\"  [PASS] Per-horizon breakdown\")\n",
        "\n",
        "    # 1.5: Combined loss\n",
        "    print(\"\\n1.5 CombinedQuantileLoss\")\n",
        "    combined_loss = CombinedQuantileLoss(quantiles)\n",
        "\n",
        "    loss_dict = combined_loss(predictions, targets)\n",
        "    print(f\"  Total loss: {loss_dict['total'].item():.6f}\")\n",
        "    print(f\"  Pinball loss: {loss_dict['pinball'].item():.6f}\")\n",
        "    assert 'total' in loss_dict and 'pinball' in loss_dict and 'crossing' in loss_dict\n",
        "    print(f\"  Crossing loss: {loss_dict['crossing'].item():.6f}\")\n",
        "    print(\"  [PASS] Combined loss components\")\n",
        "\n",
        "    # 1.6: Metrics computation\n",
        "    print(\"\\n1.6 Metrics computation\")\n",
        "    metrics = combined_loss.get_metrics(predictions, targets)\n",
        "    print(f\"  PICP 80%: {metrics['picp_80']:.3f}\")\n",
        "    print(f\"  Interval 80 mean: {metrics['interval_80_mean']:.4f}\")\n",
        "    assert 'picp_80' in metrics and 'coverage_q50' in metrics\n",
        "    print(\"  [PASS] Metrics computed\")\n",
        "\n",
        "    # 1.7: Gradient flow\n",
        "    print(\"\\n1.7 Gradient flow through loss\")\n",
        "    predictions = torch.randn(B, H, Q, requires_grad=True)\n",
        "    targets = torch.randn(B, H)\n",
        "\n",
        "    loss = loss_fn(predictions, targets)\n",
        "    loss.backward()\n",
        "\n",
        "    assert predictions.grad is not None, \"Gradients not computed\"\n",
        "    assert not torch.isnan(predictions.grad).any(), \"NaN gradients\"\n",
        "    print(f\"  Gradient norm: {predictions.grad.norm().item():.6f}\")\n",
        "    print(\"  [PASS] Gradients flow correctly\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"TEST 1 COMPLETE: All loss function tests passed\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "test_loss_functions()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQ8nHHMSA64h"
      },
      "source": [
        "## Test 2: Learning Rate Scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ujq0ng1vA64h",
        "outputId": "2187fcd0-2cd6-4719-dfc3-62a3a7ea0b4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "TEST 2: Learning Rate Scheduler\n",
            "============================================================\n",
            "\n",
            "2.1 Warmup phase\n",
            "  LR at step 0: 1.00e-06\n",
            "  LR at step 50: 5.10e-05\n",
            "  LR at step 99: 1.00e-04\n",
            "  [PASS] Linear warmup\n",
            "\n",
            "2.2 Cosine annealing phase\n",
            "  LR at epoch 0: 9.76e-05\n",
            "  LR at epoch 8 (near end of first cycle): 3.42e-06\n",
            "  LR at epoch 9 (restart): 1.00e-04\n",
            "  LR at epoch 29: 1.00e-04\n",
            "  [PASS] Cosine annealing with restarts\n",
            "\n",
            "2.3 State dict save/load\n",
            "  [PASS] State dict save/load\n",
            "\n",
            "============================================================\n",
            "TEST 2 COMPLETE: All scheduler tests passed\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "def test_scheduler():\n",
        "    \"\"\"Test learning rate scheduling with warmup and cosine annealing.\"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"TEST 2: Learning Rate Scheduler\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Create dummy model and optimizer\n",
        "    model = nn.Linear(10, 10)\n",
        "    base_lr = 1e-4\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=base_lr)\n",
        "\n",
        "    # 2.1: Warmup phase\n",
        "    print(\"\\n2.1 Warmup phase\")\n",
        "    scheduler = WarmupCosineScheduler(\n",
        "        optimizer,\n",
        "        warmup_steps=100,\n",
        "        t_0=10,\n",
        "        t_mult=2,\n",
        "        eta_min=1e-6\n",
        "    )\n",
        "\n",
        "    # Simulate warmup\n",
        "    warmup_lrs = []\n",
        "    for step in range(100):\n",
        "        scheduler.step_batch()\n",
        "        warmup_lrs.append(optimizer.param_groups[0]['lr'])\n",
        "\n",
        "    print(f\"  LR at step 0: {warmup_lrs[0]:.2e}\")\n",
        "    print(f\"  LR at step 50: {warmup_lrs[50]:.2e}\")\n",
        "    print(f\"  LR at step 99: {warmup_lrs[99]:.2e}\")\n",
        "\n",
        "    # Verify linear increase\n",
        "    assert warmup_lrs[0] < warmup_lrs[50] < warmup_lrs[99], \"LR should increase during warmup\"\n",
        "    assert abs(warmup_lrs[99] - base_lr) < 1e-6, f\"LR should reach base_lr at warmup end\"\n",
        "    print(\"  [PASS] Linear warmup\")\n",
        "\n",
        "    # 2.2: Cosine annealing\n",
        "    print(\"\\n2.2 Cosine annealing phase\")\n",
        "\n",
        "    epoch_lrs = []\n",
        "    for epoch in range(30):\n",
        "        scheduler.step()\n",
        "        epoch_lrs.append(optimizer.param_groups[0]['lr'])\n",
        "\n",
        "    print(f\"  LR at epoch 0: {epoch_lrs[0]:.2e}\")\n",
        "    print(f\"  LR at epoch 8 (near end of first cycle): {epoch_lrs[8]:.2e}\")\n",
        "    print(f\"  LR at epoch 9 (restart): {epoch_lrs[9]:.2e}\")\n",
        "    print(f\"  LR at epoch 29: {epoch_lrs[29]:.2e}\")\n",
        "\n",
        "    # Verify cosine decay\n",
        "    assert epoch_lrs[0] > epoch_lrs[5], \"LR should decrease in first cycle\"\n",
        "    assert epoch_lrs[8] < epoch_lrs[9], \"LR should jump at restart (end of epoch 9)\"\n",
        "    print(\"  [PASS] Cosine annealing with restarts\")\n",
        "\n",
        "    # 2.3: State dict save/load\n",
        "    print(\"\\n2.3 State dict save/load\")\n",
        "    state = scheduler.state_dict()\n",
        "\n",
        "    # Create new scheduler and load state\n",
        "    optimizer2 = torch.optim.AdamW(model.parameters(), lr=base_lr)\n",
        "    scheduler2 = WarmupCosineScheduler(\n",
        "        optimizer2, warmup_steps=100, t_0=10, t_mult=2, eta_min=1e-6\n",
        "    )\n",
        "    scheduler2.load_state_dict(state)\n",
        "\n",
        "    assert scheduler2.warmup_finished == scheduler.warmup_finished\n",
        "    assert scheduler2.epoch_in_cycle == scheduler.epoch_in_cycle\n",
        "    print(\"  [PASS] State dict save/load\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"TEST 2 COMPLETE: All scheduler tests passed\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "test_scheduler()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G30QuguyA64j"
      },
      "source": [
        "## Test 3: Training Step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "K650Y3MJA64j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "277d9298-961a-4027-c690-3cbf6b5f42a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "TEST 3: Training Step\n",
            "============================================================\n",
            "\n",
            "3.1 Forward pass\n",
            "  Output shape: torch.Size([4, 5, 7])\n",
            "  [PASS] Forward pass shape\n",
            "\n",
            "3.2 Loss computation\n",
            "  Loss value: 1.971276\n",
            "  [PASS] Loss computed\n",
            "\n",
            "3.3 Backward pass\n",
            "  Parameters with gradients: 213\n",
            "  Sample gradient norms:\n",
            "    revin.gamma: 0.000809\n",
            "    revin.beta: 0.004268\n",
            "    input_embedding.variable_embed.projections.0.weight: 0.000180\n",
            "  [PASS] Gradients computed\n",
            "\n",
            "3.4 Optimizer step\n",
            "  Weight change (mean abs): 0.00009992\n",
            "  [PASS] Optimizer step updated weights\n",
            "\n",
            "3.5 Mixed precision training\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3948160782.py:99: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n",
            "/tmp/ipython-input-3948160782.py:103: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  AMP loss: 1.875295\n",
            "  Scaler scale: 65536.0\n",
            "  [PASS] Mixed precision training\n",
            "\n",
            "============================================================\n",
            "TEST 3 COMPLETE: All training step tests passed\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "def test_training_step():\n",
        "    \"\"\"Test single training step with gradient flow.\"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"TEST 3: Training Step\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Create model\n",
        "    model = MIGT_TVDT(model_config['model']).to(device)\n",
        "    model.train()\n",
        "\n",
        "    # Create optimizer\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=train_config['optimizer']['lr'],\n",
        "        weight_decay=train_config['optimizer']['weight_decay']\n",
        "    )\n",
        "\n",
        "    # Loss function\n",
        "    loss_fn = CombinedQuantileLoss(\n",
        "        quantiles=train_config['quantile_regression']['quantiles']\n",
        "    ).to(device)\n",
        "\n",
        "    # Create synthetic batch\n",
        "    T, V = 288, model_config['model']['n_variables']\n",
        "    batch = {\n",
        "        'features': torch.randn(B, T, V, device=device),\n",
        "        'attention_mask': torch.ones(B, T, dtype=torch.bool, device=device),\n",
        "        'targets': torch.randn(B, H, device=device),\n",
        "        'bar_in_day': torch.arange(T).unsqueeze(0).expand(B, -1).to(device),\n",
        "        'day_of_week': torch.randint(0, 5, (B,), device=device),\n",
        "        'day_of_month': torch.randint(1, 32, (B,), device=device),\n",
        "        'day_of_year': torch.randint(1, 366, (B,), device=device)\n",
        "    }\n",
        "\n",
        "    # 3.1: Forward pass\n",
        "    print(\"\\n3.1 Forward pass\")\n",
        "    temporal_info = {\n",
        "        'bar_in_day': batch['bar_in_day'],\n",
        "        'day_of_week': batch['day_of_week'],\n",
        "        'day_of_month': batch['day_of_month'],\n",
        "        'day_of_year': batch['day_of_year']\n",
        "    }\n",
        "\n",
        "    outputs = model(\n",
        "        features=batch['features'],\n",
        "        attention_mask=batch['attention_mask'],\n",
        "        temporal_info=temporal_info\n",
        "    )\n",
        "\n",
        "    print(f\"  Output shape: {outputs['quantiles'].shape}\")\n",
        "    assert outputs['quantiles'].shape == (B, H, Q), f\"Expected ({B}, {H}, {Q})\"\n",
        "    print(\"  [PASS] Forward pass shape\")\n",
        "\n",
        "    # 3.2: Loss computation\n",
        "    print(\"\\n3.2 Loss computation\")\n",
        "    loss_dict = loss_fn(outputs['quantiles'], batch['targets'])\n",
        "    loss = loss_dict['total']\n",
        "\n",
        "    print(f\"  Loss value: {loss.item():.6f}\")\n",
        "    assert not torch.isnan(loss), \"Loss should not be NaN\"\n",
        "    assert loss.item() > 0, \"Loss should be positive\"\n",
        "    print(\"  [PASS] Loss computed\")\n",
        "\n",
        "    # 3.3: Backward pass\n",
        "    print(\"\\n3.3 Backward pass\")\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    # Check gradients exist\n",
        "    grad_norms = []\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.grad is not None:\n",
        "            grad_norms.append((name, param.grad.norm().item()))\n",
        "\n",
        "    print(f\"  Parameters with gradients: {len(grad_norms)}\")\n",
        "    print(f\"  Sample gradient norms:\")\n",
        "    for name, norm in grad_norms[:3]:\n",
        "        print(f\"    {name}: {norm:.6f}\")\n",
        "\n",
        "    assert len(grad_norms) > 0, \"No gradients computed\"\n",
        "    print(\"  [PASS] Gradients computed\")\n",
        "\n",
        "    # 3.4: Optimizer step\n",
        "    print(\"\\n3.4 Optimizer step\")\n",
        "\n",
        "    # Get initial weights\n",
        "    initial_weight = model.output_pool[0].weight.clone()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    # Check weights changed\n",
        "    weight_diff = (model.output_pool[0].weight - initial_weight).abs().mean()\n",
        "    print(f\"  Weight change (mean abs): {weight_diff.item():.8f}\")\n",
        "    assert weight_diff > 0, \"Weights should change after optimizer step\"\n",
        "    print(\"  [PASS] Optimizer step updated weights\")\n",
        "\n",
        "    # 3.5: Mixed precision\n",
        "    print(\"\\n3.5 Mixed precision training\")\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    with torch.cuda.amp.autocast():\n",
        "        outputs = model(\n",
        "            features=batch['features'],\n",
        "            attention_mask=batch['attention_mask'],\n",
        "            temporal_info=temporal_info\n",
        "        )\n",
        "        loss_dict = loss_fn(outputs['quantiles'], batch['targets'])\n",
        "        loss = loss_dict['total']\n",
        "\n",
        "    scaler.scale(loss).backward()\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "\n",
        "    print(f\"  AMP loss: {loss.item():.6f}\")\n",
        "    print(f\"  Scaler scale: {scaler.get_scale():.1f}\")\n",
        "    print(\"  [PASS] Mixed precision training\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"TEST 3 COMPLETE: All training step tests passed\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "test_training_step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gB-0BjwvA64k"
      },
      "source": [
        "## Test 4: Validation Step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "QI591QhtA64l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "437bc149-37ee-434c-d9a9-c061ddb41518"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "TEST 4: Validation Step\n",
            "============================================================\n",
            "\n",
            "4.1 Validation forward pass\n",
            "  Val loss: 1.865381\n",
            "  [PASS] No-gradient validation\n",
            "\n",
            "4.2 Detailed metrics\n",
            "  PICP 80%: 0.050\n",
            "  Coverage q50: 1.000\n",
            "  Interval 80 mean: 2.7321\n",
            "  Loss 15m: 1.845668\n",
            "  [PASS] All metrics computed\n",
            "\n",
            "4.3 Quantile non-crossing verification\n",
            "  Min quantile diff: 0.600559\n",
            "  All non-crossing: True\n",
            "  [PASS] Non-crossing quantiles verified\n",
            "\n",
            "============================================================\n",
            "TEST 4 COMPLETE: All validation tests passed\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "def test_validation_step():\n",
        "    \"\"\"Test validation with metrics computation.\"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"TEST 4: Validation Step\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    model = MIGT_TVDT(model_config['model']).to(device)\n",
        "    model.eval()\n",
        "\n",
        "    loss_fn = CombinedQuantileLoss(\n",
        "        quantiles=train_config['quantile_regression']['quantiles']\n",
        "    ).to(device)\n",
        "\n",
        "    # Create synthetic validation batch\n",
        "    T, V = 288, model_config['model']['n_variables']\n",
        "    batch = {\n",
        "        'features': torch.randn(B, T, V, device=device),\n",
        "        'attention_mask': torch.ones(B, T, dtype=torch.bool, device=device),\n",
        "        'targets': torch.randn(B, H, device=device),\n",
        "        'bar_in_day': torch.arange(T).unsqueeze(0).expand(B, -1).to(device),\n",
        "        'day_of_week': torch.randint(0, 5, (B,), device=device),\n",
        "        'day_of_month': torch.randint(1, 32, (B,), device=device),\n",
        "        'day_of_year': torch.randint(1, 366, (B,), device=device)\n",
        "    }\n",
        "\n",
        "    # 4.1: Validation forward pass (no gradients)\n",
        "    print(\"\\n4.1 Validation forward pass\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        temporal_info = {\n",
        "            'bar_in_day': batch['bar_in_day'],\n",
        "            'day_of_week': batch['day_of_week'],\n",
        "            'day_of_month': batch['day_of_month'],\n",
        "            'day_of_year': batch['day_of_year']\n",
        "        }\n",
        "\n",
        "        outputs = model(\n",
        "            features=batch['features'],\n",
        "            attention_mask=batch['attention_mask'],\n",
        "            temporal_info=temporal_info\n",
        "        )\n",
        "\n",
        "        loss_dict = loss_fn(outputs['quantiles'], batch['targets'])\n",
        "\n",
        "    print(f\"  Val loss: {loss_dict['total'].item():.6f}\")\n",
        "    print(\"  [PASS] No-gradient validation\")\n",
        "\n",
        "    # 4.2: Metrics computation\n",
        "    print(\"\\n4.2 Detailed metrics\")\n",
        "\n",
        "    predictions = outputs['quantiles'].cpu()\n",
        "    targets = batch['targets'].cpu()\n",
        "\n",
        "    metrics = loss_fn.get_metrics(predictions, targets)\n",
        "\n",
        "    print(f\"  PICP 80%: {metrics['picp_80']:.3f}\")\n",
        "    print(f\"  Coverage q50: {metrics['coverage_q50']:.3f}\")\n",
        "    print(f\"  Interval 80 mean: {metrics['interval_80_mean']:.4f}\")\n",
        "    print(f\"  Loss 15m: {metrics['loss_15m']:.6f}\")\n",
        "\n",
        "    # Check all expected metrics present\n",
        "    expected_keys = ['picp_80', 'coverage_q50', 'interval_80_mean', 'loss_15m']\n",
        "    for key in expected_keys:\n",
        "        assert key in metrics, f\"Missing metric: {key}\"\n",
        "    print(\"  [PASS] All metrics computed\")\n",
        "\n",
        "    # 4.3: Non-crossing verification\n",
        "    print(\"\\n4.3 Quantile non-crossing verification\")\n",
        "\n",
        "    # Check all quantiles are monotonically increasing\n",
        "    diffs = predictions[:, :, 1:] - predictions[:, :, :-1]\n",
        "    all_positive = (diffs >= 0).all()\n",
        "\n",
        "    print(f\"  Min quantile diff: {diffs.min().item():.6f}\")\n",
        "    print(f\"  All non-crossing: {all_positive.item()}\")\n",
        "    assert all_positive, \"Quantiles should be non-crossing\"\n",
        "    print(\"  [PASS] Non-crossing quantiles verified\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"TEST 4 COMPLETE: All validation tests passed\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "test_validation_step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFlD2HKGA64l"
      },
      "source": [
        "## Test 5: Full Training Loop (Mini)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "sRv7vIq7A64l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64498de6-983a-4cef-cc82-95b3ea57d067"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "TEST 5: Full Training Loop (Mini)\n",
            "============================================================\n",
            "\n",
            "5.1 Create trainer\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/Transformers/FP/src/training/trainer.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = GradScaler(enabled=self.use_amp)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [PASS] Trainer created\n",
            "\n",
            "5.2 Run training (3 epochs)\n",
            "Starting training on cuda\n",
            "  Model parameters: 6,866,984\n",
            "  Training samples: 16\n",
            "  Validation samples: 16\n",
            "  Mixed precision: True\n",
            "  Gradient accumulation: 1\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 [Train]:   0%|          | 0/4 [00:00<?, ?it/s]/content/drive/MyDrive/Colab Notebooks/Transformers/FP/src/training/trainer.py:355: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=self.use_amp):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3 | Train: 1.811052 | Val: 1.726491 | PICP80: 0.125 | LR: 4.00e-05\n",
            "  New best model saved (val_loss: 1.726491)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/3 | Train: 1.685124 | Val: 1.534801 | PICP80: 0.213 | LR: 8.00e-05\n",
            "  New best model saved (val_loss: 1.534801)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/3 | Train: 1.433827 | Val: 1.216600 | PICP80: 0.412 | LR: 1.00e-04\n",
            "  New best model saved (val_loss: 1.216600)\n",
            "\n",
            "Training complete in 0.00 hours\n",
            "Best validation loss: 1.216600\n",
            "Training history saved to /content/test_outputs/training_history.json\n",
            "\n",
            "  Final train loss: 1.433827\n",
            "  Final val loss: 1.216600\n",
            "  Best val loss: 1.216600\n",
            "  [PASS] Training completed\n",
            "\n",
            "5.3 Loss trend\n",
            "  Train losses: [1.8110524117946625, 1.6851240992546082, 1.4338270723819733]\n",
            "  [PASS] All losses finite\n",
            "\n",
            "5.4 Output files\n",
            "  checkpoint_latest.pt: exists\n",
            "  checkpoint_best.pt: exists\n",
            "  training_history.json: exists\n",
            "  [PASS] All output files created\n",
            "\n",
            "============================================================\n",
            "TEST 5 COMPLETE: Full training loop test passed\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "def test_training_loop_mini():\n",
        "    \"\"\"Test full training loop with synthetic data.\"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"TEST 5: Full Training Loop (Mini)\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Create synthetic dataset\n",
        "    from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "    T, V = 288, model_config['model']['n_variables']\n",
        "    n_samples = 32\n",
        "\n",
        "    features = torch.randn(n_samples, T, V)\n",
        "    masks = torch.ones(n_samples, T, dtype=torch.bool)\n",
        "    targets = torch.randn(n_samples, H)\n",
        "    bar_in_day = torch.arange(T).unsqueeze(0).expand(n_samples, -1)\n",
        "    day_of_week = torch.randint(0, 5, (n_samples,))\n",
        "    day_of_month = torch.randint(1, 32, (n_samples,))\n",
        "    day_of_year = torch.randint(1, 366, (n_samples,))\n",
        "\n",
        "    # Custom collate for synthetic data\n",
        "    def synthetic_collate(batch):\n",
        "        indices = torch.tensor(batch)\n",
        "        return {\n",
        "            'features': features[indices],\n",
        "            'attention_mask': masks[indices],\n",
        "            'targets': targets[indices],\n",
        "            'bar_in_day': bar_in_day[indices],\n",
        "            'day_of_week': day_of_week[indices],\n",
        "            'day_of_month': day_of_month[indices],\n",
        "            'day_of_year': day_of_year[indices]\n",
        "        }\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        list(range(n_samples // 2)),\n",
        "        batch_size=4,\n",
        "        shuffle=True,\n",
        "        collate_fn=synthetic_collate\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        list(range(n_samples // 2, n_samples)),\n",
        "        batch_size=4,\n",
        "        collate_fn=synthetic_collate\n",
        "    )\n",
        "\n",
        "    # Create model and trainer config\n",
        "    model = MIGT_TVDT(model_config['model']).to(device)\n",
        "\n",
        "    mini_config = {\n",
        "        'training': {\n",
        "            'batch_size': 4,\n",
        "            'gradient_accumulation_steps': 1,\n",
        "            'max_epochs': 3,\n",
        "            'early_stopping_patience': 5,\n",
        "            'mixed_precision': True\n",
        "        },\n",
        "        'optimizer': {\n",
        "            'lr': 1e-4,\n",
        "            'weight_decay': 0.01,\n",
        "            'betas': [0.9, 0.999]\n",
        "        },\n",
        "        'scheduler': {\n",
        "            'warmup_steps': 10,\n",
        "            't_0': 2,\n",
        "            't_mult': 2,\n",
        "            'eta_min': 1e-6\n",
        "        },\n",
        "        'regularization': {\n",
        "            'gradient_clip_norm': 1.0\n",
        "        },\n",
        "        'quantile_regression': {\n",
        "            'quantiles': [0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95],\n",
        "            'crossing_weight': 0.0\n",
        "        },\n",
        "        'checkpointing': {\n",
        "            'save_top_k': 2\n",
        "        }\n",
        "    }\n",
        "\n",
        "    output_dir = Path('/content/test_outputs')\n",
        "    output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    # 5.1: Create trainer\n",
        "    print(\"\\n5.1 Create trainer\")\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        config=mini_config,\n",
        "        train_loader=train_loader,\n",
        "        val_loader=val_loader,\n",
        "        output_dir=output_dir,\n",
        "        device=device\n",
        "    )\n",
        "    print(\"  [PASS] Trainer created\")\n",
        "\n",
        "    # 5.2: Run training\n",
        "    print(\"\\n5.2 Run training (3 epochs)\")\n",
        "    history = trainer.train()\n",
        "\n",
        "    print(f\"\\n  Final train loss: {history['train_loss'][-1]:.6f}\")\n",
        "    print(f\"  Final val loss: {history['val_loss'][-1]:.6f}\")\n",
        "    print(f\"  Best val loss: {trainer.best_val_loss:.6f}\")\n",
        "\n",
        "    assert len(history['train_loss']) == 3, \"Should have 3 epochs\"\n",
        "    assert len(history['val_loss']) == 3\n",
        "    print(\"  [PASS] Training completed\")\n",
        "\n",
        "    # 5.3: Verify loss decreased (with high probability)\n",
        "    print(\"\\n5.3 Loss trend\")\n",
        "    # Note: With random data, loss may not always decrease\n",
        "    # We just verify no NaN/inf\n",
        "    for loss in history['train_loss']:\n",
        "        assert np.isfinite(loss), \"Loss should be finite\"\n",
        "    print(f\"  Train losses: {history['train_loss']}\")\n",
        "    print(\"  [PASS] All losses finite\")\n",
        "\n",
        "    # 5.4: Check files created\n",
        "    print(\"\\n5.4 Output files\")\n",
        "    assert (output_dir / 'checkpoint_latest.pt').exists()\n",
        "    assert (output_dir / 'checkpoint_best.pt').exists()\n",
        "    assert (output_dir / 'training_history.json').exists()\n",
        "    print(\"  checkpoint_latest.pt: exists\")\n",
        "    print(\"  checkpoint_best.pt: exists\")\n",
        "    print(\"  training_history.json: exists\")\n",
        "    print(\"  [PASS] All output files created\")\n",
        "\n",
        "    # Cleanup\n",
        "    import shutil\n",
        "    shutil.rmtree(output_dir)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"TEST 5 COMPLETE: Full training loop test passed\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "test_training_loop_mini()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlSDWuqGA64m"
      },
      "source": [
        "## Test 6: Checkpoint Save/Load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "CIvfZV5DA64m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20c89fa2-eab8-4ed6-ccde-8c79331dab58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "TEST 6: Checkpoint Save/Load\n",
            "============================================================\n",
            "\n",
            "6.1 Save checkpoint\n",
            "  Saved to: /content/test_checkpoint/test_checkpoint.pt\n",
            "  File size: 27.55 MB\n",
            "  [PASS] Checkpoint saved\n",
            "\n",
            "6.2 Load checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3905555930.py:14: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n",
            "/tmp/ipython-input-3905555930.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler2 = torch.cuda.amp.GradScaler()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Loaded epoch: 5\n",
            "  Loaded global_step: 500\n",
            "  Loaded val_loss: 0.123\n",
            "  [PASS] Checkpoint loaded\n",
            "\n",
            "6.3 Verify model outputs match\n",
            "  Max output difference: 0.0000000000\n",
            "  [PASS] Model outputs match\n",
            "\n",
            "============================================================\n",
            "TEST 6 COMPLETE: Checkpoint save/load test passed\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "def test_checkpoint_save_load():\n",
        "    \"\"\"Test checkpoint saving and loading.\"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"TEST 6: Checkpoint Save/Load\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    output_dir = Path('/content/test_checkpoint')\n",
        "    output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    # Create and initialize model\n",
        "    model = MIGT_TVDT(model_config['model']).to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
        "    scheduler = WarmupCosineScheduler(optimizer, warmup_steps=100)\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "    # Simulate some training\n",
        "    for _ in range(50):\n",
        "        scheduler.step_batch()\n",
        "\n",
        "    # 6.1: Save checkpoint\n",
        "    print(\"\\n6.1 Save checkpoint\")\n",
        "    checkpoint = {\n",
        "        'epoch': 5,\n",
        "        'global_step': 500,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict': scheduler.state_dict(),\n",
        "        'scaler_state_dict': scaler.state_dict(),\n",
        "        'val_loss': 0.123,\n",
        "        'best_val_loss': 0.100,\n",
        "        'config': model_config\n",
        "    }\n",
        "\n",
        "    checkpoint_path = output_dir / 'test_checkpoint.pt'\n",
        "    torch.save(checkpoint, checkpoint_path)\n",
        "    print(f\"  Saved to: {checkpoint_path}\")\n",
        "    print(f\"  File size: {checkpoint_path.stat().st_size / 1e6:.2f} MB\")\n",
        "    print(\"  [PASS] Checkpoint saved\")\n",
        "\n",
        "    # 6.2: Load checkpoint into new model\n",
        "    print(\"\\n6.2 Load checkpoint\")\n",
        "\n",
        "    model2 = MIGT_TVDT(model_config['model']).to(device)\n",
        "    optimizer2 = torch.optim.AdamW(model2.parameters(), lr=1e-4)\n",
        "    scheduler2 = WarmupCosineScheduler(optimizer2, warmup_steps=100)\n",
        "    scaler2 = torch.cuda.amp.GradScaler()\n",
        "\n",
        "    loaded = torch.load(checkpoint_path, map_location=device)\n",
        "\n",
        "    model2.load_state_dict(loaded['model_state_dict'])\n",
        "    optimizer2.load_state_dict(loaded['optimizer_state_dict'])\n",
        "    scheduler2.load_state_dict(loaded['scheduler_state_dict'])\n",
        "    scaler2.load_state_dict(loaded['scaler_state_dict'])\n",
        "\n",
        "    print(f\"  Loaded epoch: {loaded['epoch']}\")\n",
        "    print(f\"  Loaded global_step: {loaded['global_step']}\")\n",
        "    print(f\"  Loaded val_loss: {loaded['val_loss']}\")\n",
        "    print(\"  [PASS] Checkpoint loaded\")\n",
        "\n",
        "    # 6.3: Verify model outputs match\n",
        "    print(\"\\n6.3 Verify model outputs match\")\n",
        "\n",
        "    model.eval()\n",
        "    model2.eval()\n",
        "\n",
        "    T, V = 288, model_config['model']['n_variables']\n",
        "    test_input = torch.randn(1, T, V, device=device)\n",
        "    test_mask = torch.ones(1, T, dtype=torch.bool, device=device)\n",
        "    temporal_info = {\n",
        "        'bar_in_day': torch.arange(T).unsqueeze(0).to(device),\n",
        "        'day_of_week': torch.tensor([0], device=device),\n",
        "        'day_of_month': torch.tensor([1], device=device),\n",
        "        'day_of_year': torch.tensor([1], device=device)\n",
        "    }\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out1 = model(test_input, test_mask, temporal_info)['quantiles']\n",
        "        out2 = model2(test_input, test_mask, temporal_info)['quantiles']\n",
        "\n",
        "    diff = (out1 - out2).abs().max().item()\n",
        "    print(f\"  Max output difference: {diff:.10f}\")\n",
        "    assert diff < 1e-6, f\"Outputs should match, got diff={diff}\"\n",
        "    print(\"  [PASS] Model outputs match\")\n",
        "\n",
        "    # Cleanup\n",
        "    import shutil\n",
        "    shutil.rmtree(output_dir)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"TEST 6 COMPLETE: Checkpoint save/load test passed\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "test_checkpoint_save_load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yp0gIYjUA64n"
      },
      "source": [
        "## Test 7: Early Stopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "4cdFTEmxA64n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cbc6bfd-cf49-4401-8054-d878d1df923e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "TEST 7: Early Stopping\n",
            "============================================================\n",
            "\n",
            "7.1 Basic early stopping (patience=3)\n",
            "  Epoch 0: score=1.00, counter=0, stop=False\n",
            "  Epoch 1: score=0.90, counter=0, stop=False\n",
            "  Epoch 2: score=0.80, counter=0, stop=False\n",
            "  Epoch 3: score=0.80, counter=1, stop=False\n",
            "  Epoch 4: score=0.80, counter=2, stop=False\n",
            "  Epoch 5: score=0.80, counter=3, stop=True\n",
            "  Stopped at epoch 5\n",
            "  [PASS] Early stopping triggered\n",
            "\n",
            "7.2 No stopping with improvement\n",
            "  Final best score: 0.50\n",
            "  [PASS] No early stopping with improvement\n",
            "\n",
            "7.3 Max mode (accuracy-like)\n",
            "  Best score: 0.65\n",
            "  [PASS] Max mode early stopping\n",
            "\n",
            "============================================================\n",
            "TEST 7 COMPLETE: Early stopping tests passed\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "def test_early_stopping():\n",
        "    \"\"\"Test early stopping behavior.\"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"TEST 7: Early Stopping\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # 7.1: Basic early stopping\n",
        "    print(\"\\n7.1 Basic early stopping (patience=3)\")\n",
        "\n",
        "    es = EarlyStopping(patience=3, min_delta=0.001, mode='min')\n",
        "\n",
        "    # Simulate improving then stagnating\n",
        "    scores = [1.0, 0.9, 0.8, 0.8, 0.8, 0.8]  # Stagnates after epoch 2\n",
        "\n",
        "    for i, score in enumerate(scores):\n",
        "        should_stop = es(score)\n",
        "        print(f\"  Epoch {i}: score={score:.2f}, counter={es.counter}, stop={should_stop}\")\n",
        "\n",
        "        if should_stop:\n",
        "            print(f\"  Stopped at epoch {i}\")\n",
        "            break\n",
        "\n",
        "    assert should_stop, \"Should have triggered early stopping\"\n",
        "    print(\"  [PASS] Early stopping triggered\")\n",
        "\n",
        "    # 7.2: No stopping with improvement\n",
        "    print(\"\\n7.2 No stopping with improvement\")\n",
        "\n",
        "    es2 = EarlyStopping(patience=3, min_delta=0.001, mode='min')\n",
        "    scores2 = [1.0, 0.9, 0.8, 0.7, 0.6, 0.5]\n",
        "\n",
        "    for i, score in enumerate(scores2):\n",
        "        should_stop = es2(score)\n",
        "        if should_stop:\n",
        "            print(f\"  Unexpectedly stopped at epoch {i}\")\n",
        "            break\n",
        "\n",
        "    assert not should_stop, \"Should not have stopped\"\n",
        "    print(f\"  Final best score: {es2.best_score:.2f}\")\n",
        "    print(\"  [PASS] No early stopping with improvement\")\n",
        "\n",
        "    # 7.3: Max mode (for accuracy-like metrics)\n",
        "    print(\"\\n7.3 Max mode (accuracy-like)\")\n",
        "\n",
        "    es3 = EarlyStopping(patience=2, min_delta=0.01, mode='max')\n",
        "    scores3 = [0.5, 0.6, 0.65, 0.65, 0.65]\n",
        "\n",
        "    for i, score in enumerate(scores3):\n",
        "        should_stop = es3(score)\n",
        "        if should_stop:\n",
        "            break\n",
        "\n",
        "    assert should_stop, \"Should stop in max mode\"\n",
        "    print(f\"  Best score: {es3.best_score:.2f}\")\n",
        "    print(\"  [PASS] Max mode early stopping\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"TEST 7 COMPLETE: Early stopping tests passed\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "test_early_stopping()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VID8MyoA64o"
      },
      "source": [
        "## Test 8: Phase 3/4 Integration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "HJj2pd7KA64o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70363b66-f142-4300-9661-ccd56960e422"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "TEST 8: Phase 3/4 Integration\n",
            "============================================================\n",
            "\n",
            "8.1 Load NQDataModule\n",
            "Loading data from /content/drive/MyDrive/Colab Notebooks/Transformers/FP/data/processed/nq_features_full.parquet\n",
            "Features: 24\n",
            "Targets: 5\n",
            "Split statistics:\n",
            "  Train: 808,996 samples (2010-06-07 to 2021-12-31)\n",
            "  Val:   141,516 samples (2022-01-02 to 2023-12-29)\n",
            "  Test:  136,284 samples (2024-01-02 to 2025-12-03)\n",
            "\n",
            "Temporal gaps:\n",
            "  Train-Val gap: 49.1 hours\n",
            "  Val-Test gap: 74.1 hours\n",
            "  Purged samples: ~576 total (~288 per gap)\n",
            "[PASS] No data leakage detected:\n",
            "  Train-Val gap: 49.1 hours\n",
            "  Val-Test gap: 74.1 hours\n",
            "\n",
            "Dataset sizes:\n",
            "  Train: 808,708\n",
            "  Val:   141,228\n",
            "  Test:  135,996\n",
            "  Train samples: 808,708\n",
            "  Val samples: 141,228\n",
            "  Test samples: 135,996\n",
            "  [PASS] Data module loaded\n",
            "\n",
            "8.2 Get batch from dataloader\n",
            "  features shape: torch.Size([4, 288, 24])\n",
            "  attention_mask shape: torch.Size([4, 288])\n",
            "  targets shape: torch.Size([4, 5])\n",
            "  bar_in_day shape: torch.Size([4, 288])\n",
            "  [PASS] Batch retrieved\n",
            "\n",
            "8.3 Forward pass with real data\n",
            "  Output shape: torch.Size([4, 5, 7])\n",
            "  Output sample (first horizon):\n",
            "tensor([0.5451, 1.1988, 1.8350, 2.5532, 3.2357, 3.9715, 4.6398],\n",
            "       device='cuda:0')\n",
            "  [PASS] Forward pass successful\n",
            "\n",
            "8.4 Loss computation with real data\n",
            "  Total loss: 1.837522\n",
            "  Pinball loss: 1.837522\n",
            "  [PASS] Loss computed successfully\n",
            "\n",
            "8.5 Metrics with real data\n",
            "  PICP 80%: 0.000\n",
            "  Coverage q50: 1.000\n",
            "  Interval 80 mean: 2.798295\n",
            "  [PASS] Metrics computed\n",
            "\n",
            "============================================================\n",
            "TEST 8 COMPLETE: Phase integration tests passed\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "def test_phase_integration():\n",
        "    \"\"\"Test integration with Phase 3 data pipeline and Phase 4 model.\"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"TEST 8: Phase 3/4 Integration\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Check for processed data\n",
        "    data_path = BASE_DIR / 'data/processed/nq_features_full.parquet'\n",
        "\n",
        "    if not data_path.exists():\n",
        "        print(f\"\\n  [SKIP] Data file not found: {data_path}\")\n",
        "        print(\"  Run Phase 3 preprocessing first.\")\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"TEST 8 SKIPPED: Data not available\")\n",
        "        print(\"=\" * 60)\n",
        "        return\n",
        "\n",
        "    # 8.1: Load data module\n",
        "    print(\"\\n8.1 Load NQDataModule\")\n",
        "\n",
        "    data_module = NQDataModule(\n",
        "        data_path=data_path,\n",
        "        batch_size=4,  # Small batch for testing\n",
        "        num_workers=0,\n",
        "        pin_memory=False\n",
        "    )\n",
        "    data_module.setup()\n",
        "\n",
        "    print(f\"  Train samples: {len(data_module.train_dataset):,}\")\n",
        "    print(f\"  Val samples: {len(data_module.val_dataset):,}\")\n",
        "    print(f\"  Test samples: {len(data_module.test_dataset):,}\")\n",
        "    print(\"  [PASS] Data module loaded\")\n",
        "\n",
        "    # 8.2: Get batch from dataloader\n",
        "    print(\"\\n8.2 Get batch from dataloader\")\n",
        "\n",
        "    train_loader = data_module.train_dataloader()\n",
        "    batch = next(iter(train_loader))\n",
        "\n",
        "    print(f\"  features shape: {batch['features'].shape}\")\n",
        "    print(f\"  attention_mask shape: {batch['attention_mask'].shape}\")\n",
        "    print(f\"  targets shape: {batch['targets'].shape}\")\n",
        "    print(f\"  bar_in_day shape: {batch['bar_in_day'].shape}\")\n",
        "    print(\"  [PASS] Batch retrieved\")\n",
        "\n",
        "    # 8.3: Forward pass through model\n",
        "    print(\"\\n8.3 Forward pass with real data\")\n",
        "\n",
        "    model = MIGT_TVDT(model_config['model']).to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Move batch to device\n",
        "    batch_device = {\n",
        "        k: v.to(device) if isinstance(v, torch.Tensor) else v\n",
        "        for k, v in batch.items()\n",
        "    }\n",
        "\n",
        "    temporal_info = {\n",
        "        'bar_in_day': batch_device['bar_in_day'],\n",
        "        'day_of_week': batch_device['day_of_week'],\n",
        "        'day_of_month': batch_device['day_of_month'],\n",
        "        'day_of_year': batch_device['day_of_year']\n",
        "    }\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(\n",
        "            features=batch_device['features'],\n",
        "            attention_mask=batch_device['attention_mask'],\n",
        "            temporal_info=temporal_info\n",
        "        )\n",
        "\n",
        "    print(f\"  Output shape: {outputs['quantiles'].shape}\")\n",
        "    print(f\"  Output sample (first horizon):\\n{outputs['quantiles'][0, 0]}\")\n",
        "    print(\"  [PASS] Forward pass successful\")\n",
        "\n",
        "    # 8.4: Loss computation with real data\n",
        "    print(\"\\n8.4 Loss computation with real data\")\n",
        "\n",
        "    loss_fn = CombinedQuantileLoss(\n",
        "        quantiles=train_config['quantile_regression']['quantiles']\n",
        "    ).to(device)\n",
        "\n",
        "    loss_dict = loss_fn(outputs['quantiles'], batch_device['targets'])\n",
        "\n",
        "    print(f\"  Total loss: {loss_dict['total'].item():.6f}\")\n",
        "    print(f\"  Pinball loss: {loss_dict['pinball'].item():.6f}\")\n",
        "\n",
        "    assert loss_dict['total'].item() > 0, \"Loss should be positive\"\n",
        "    assert np.isfinite(loss_dict['total'].item()), \"Loss should be finite\"\n",
        "    print(\"  [PASS] Loss computed successfully\")\n",
        "\n",
        "    # 8.5: Metrics with real data\n",
        "    print(\"\\n8.5 Metrics with real data\")\n",
        "\n",
        "    metrics = loss_fn.get_metrics(\n",
        "        outputs['quantiles'].cpu(),\n",
        "        batch_device['targets'].cpu()\n",
        "    )\n",
        "\n",
        "    print(f\"  PICP 80%: {metrics['picp_80']:.3f}\")\n",
        "    print(f\"  Coverage q50: {metrics['coverage_q50']:.3f}\")\n",
        "    print(f\"  Interval 80 mean: {metrics['interval_80_mean']:.6f}\")\n",
        "    print(\"  [PASS] Metrics computed\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"TEST 8 COMPLETE: Phase integration tests passed\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "test_phase_integration()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRaDnyEvA64p"
      },
      "source": [
        "## Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "g3hQJSHZA64p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c98bc6ae-700b-4223-f76f-7c746236bc37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "DEV PHASE 5: TRAINING PIPELINE TESTS COMPLETE\n",
            "======================================================================\n",
            "\n",
            "All tests passed. Training pipeline is ready for full training.\n",
            "\n",
            "Delivered components:\n",
            "  - src/training/loss_functions.py\n",
            "  - src/training/scheduler.py\n",
            "  - src/training/trainer.py\n",
            "  - src/training/__init__.py\n",
            "  - configs/training_config.yaml\n",
            "\n",
            "Next steps for full training:\n",
            "  1. Ensure Phase 3 data is preprocessed\n",
            "  2. Copy data to VM: /content/data/\n",
            "  3. Run full training with Trainer class\n",
            "  4. Monitor with WandB or TensorBoard\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"DEV PHASE 5: TRAINING PIPELINE TESTS COMPLETE\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\\nAll tests passed. Training pipeline is ready for full training.\")\n",
        "print(\"\\nDelivered components:\")\n",
        "print(\"  - src/training/loss_functions.py\")\n",
        "print(\"  - src/training/scheduler.py\")\n",
        "print(\"  - src/training/trainer.py\")\n",
        "print(\"  - src/training/__init__.py\")\n",
        "print(\"  - configs/training_config.yaml\")\n",
        "print(\"\\nNext steps for full training:\")\n",
        "print(\"  1. Ensure Phase 3 data is preprocessed\")\n",
        "print(\"  2. Copy data to VM: /content/data/\")\n",
        "print(\"  3. Run full training with Trainer class\")\n",
        "print(\"  4. Monitor with WandB or TensorBoard\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}