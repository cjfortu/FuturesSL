{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ULftS6aiTdxA"
   },
   "source": [
    "# Production Model Evaluation\n",
    "\n",
    "Comprehensive evaluation of trained MIGT-TVDT model from Phase 5 outputs.\n",
    "\n",
    "**Evaluation Components:**\n",
    "1. Model loading and inference on full test set\n",
    "2. Distributional metrics (CRPS, calibration, PICP, MPIW)\n",
    "3. Point prediction metrics (IC, DA, RMSE)\n",
    "4. Financial metrics (Sharpe, Sortino, MDD, profit factor)\n",
    "5. Calibration analysis and visualization\n",
    "6. Multi-horizon backtesting\n",
    "7. Comprehensive evaluation report\n",
    "\n",
    "**Outputs:** All results saved to `/content/drive/MyDrive/Colab Notebooks/Transformers/FP/evaluation_results`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4IFKxQAwTdxD"
   },
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gBE80XeuTdxE",
    "outputId": "5019b29c-c0c7-414e-b485-8881ed34d07f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "Base directory: /content/drive/MyDrive/Colab Notebooks/Transformers/FP\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "BASE_DIR = Path('/content/drive/MyDrive/Colab Notebooks/Transformers/FP')\n",
    "sys.path.insert(0, str(BASE_DIR / 'src'))\n",
    "\n",
    "print(f'Base directory: {BASE_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "iGly_2SHTdxG"
   },
   "outputs": [],
   "source": [
    "!pip install -q scipy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qASAokHsTdxG",
    "outputId": "a57415fa-d7df-46dc-9b4a-10d84d679f1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GPU: NVIDIA A100-SXM4-80GB\n",
      "VRAM: 85.2 GB\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lDhgM8_KTdxH"
   },
   "source": [
    "## 2. Configure Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "36vF2kWYTdxH",
    "outputId": "89e63403-6553-4a1c-e05b-eb226a9cc20b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paths:\n",
      "  checkpoint: exists\n",
      "  training_history: exists\n",
      "  processed_data: exists\n",
      "  results_dir: exists\n"
     ]
    }
   ],
   "source": [
    "paths = {\n",
    "    'checkpoint': BASE_DIR / 'outputs' / 'checkpoint_best.pt',\n",
    "    'training_history': BASE_DIR / 'outputs' / 'training_history.json',\n",
    "    'processed_data': BASE_DIR / 'data' / 'processed',\n",
    "    'results_dir': BASE_DIR / 'evaluation_results'\n",
    "}\n",
    "\n",
    "paths['results_dir'].mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Paths:\")\n",
    "for key, path in paths.items():\n",
    "    status = \"exists\" if path.exists() else \"missing\"\n",
    "    print(f\"  {key}: {status}\")\n",
    "\n",
    "if not paths['checkpoint'].exists():\n",
    "    raise FileNotFoundError(f\"Checkpoint not found: {paths['checkpoint']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3zodpOkuTdxI"
   },
   "source": [
    "## 3. Load Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UgsM6QeTTdxJ",
    "outputId": "485ea668-5c72-421b-975b-aefc521d244a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training History:\n",
      "  Epochs: 10\n",
      "  Best epoch: 10\n",
      "  Best val loss: 0.001176\n",
      "  Final train loss: 0.000850\n",
      "  Final val loss: 0.001176\n"
     ]
    }
   ],
   "source": [
    "with open(paths['training_history'], 'r') as f:\n",
    "    history = json.load(f)\n",
    "\n",
    "print(\"Training History:\")\n",
    "print(f\"  Epochs: {len(history['train_loss'])}\")\n",
    "print(f\"  Best epoch: {np.argmin(history['val_loss']) + 1}\")\n",
    "print(f\"  Best val loss: {np.min(history['val_loss']):.6f}\")\n",
    "print(f\"  Final train loss: {history['train_loss'][-1]:.6f}\")\n",
    "print(f\"  Final val loss: {history['val_loss'][-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XPgPnxHBTdxL"
   },
   "source": [
    "## 4. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qHQvt4UlTdxM",
    "outputId": "42b81f5e-3436-4e16-b89b-8c038cc56846"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint: epoch 9, val_loss 0.001176\n",
      "\n",
      "Model config:\n",
      "  n_variables: 24\n",
      "  max_seq_len: 288\n",
      "  n_horizons: 5\n",
      "  n_quantiles: 7\n",
      "  d_model: 256\n",
      "\n",
      "Parameters: 6,866,984\n"
     ]
    }
   ],
   "source": [
    "from model.migt_tvdt import MIGT_TVDT\n",
    "\n",
    "checkpoint = torch.load(paths['checkpoint'], map_location=device)\n",
    "\n",
    "print(f\"Checkpoint: epoch {checkpoint['epoch']}, val_loss {checkpoint['val_loss']:.6f}\")\n",
    "\n",
    "model_config = checkpoint['config']['model']\n",
    "print(f\"\\nModel config:\")\n",
    "print(f\"  n_variables: {model_config['n_variables']}\")\n",
    "print(f\"  max_seq_len: {model_config['max_seq_len']}\")\n",
    "print(f\"  n_horizons: {model_config['n_horizons']}\")\n",
    "print(f\"  n_quantiles: {model_config['n_quantiles']}\")\n",
    "print(f\"  d_model: {model_config['d_model']}\")\n",
    "\n",
    "model = MIGT_TVDT(model_config)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nParameters: {n_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "txW0sWzhTdxM"
   },
   "source": [
    "## 5. Extract Config Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "es2pOpnOTdxN",
    "outputId": "839790a4-87d1-4033-df5e-fc5ef36e670b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantiles: [0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95]\n",
      "Horizons: ['15m', '30m', '60m', '2h', '4h']\n"
     ]
    }
   ],
   "source": [
    "# Quantiles from checkpoint config\n",
    "if 'quantiles' in checkpoint['config']:\n",
    "    quantiles = checkpoint['config']['quantiles']\n",
    "elif 'quantile_regression' in checkpoint['config']:\n",
    "    quantiles = checkpoint['config']['quantile_regression']['quantiles']\n",
    "else:\n",
    "    # Default from problem statement\n",
    "    quantiles = [0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95]\n",
    "\n",
    "# Horizon names from checkpoint or default\n",
    "if 'horizon_names' in checkpoint['config']:\n",
    "    horizon_names = checkpoint['config']['horizon_names']\n",
    "elif 'horizons' in checkpoint['config'].get('model', {}):\n",
    "    horizon_names = checkpoint['config']['model']['horizons']\n",
    "else:\n",
    "    # Default from problem statement\n",
    "    horizon_names = ['15m', '30m', '60m', '2h', '4h']\n",
    "\n",
    "print(f\"Quantiles: {quantiles}\")\n",
    "print(f\"Horizons: {horizon_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i1kj9VwUTdxN"
   },
   "source": [
    "## 6. Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kh_D3AmqTdxN",
    "outputId": "3f81eed2-70ef-4f02-b78b-ee35cc51e824"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from /content/drive/MyDrive/Colab Notebooks/Transformers/FP/data/processed/nq_features_full.parquet\n",
      "Features: 24\n",
      "Targets: 5\n",
      "Split statistics:\n",
      "  Train: 808,996 samples (2010-06-07 to 2021-12-31)\n",
      "  Val:   141,516 samples (2022-01-02 to 2023-12-29)\n",
      "  Test:  136,284 samples (2024-01-02 to 2025-12-03)\n",
      "\n",
      "Temporal gaps:\n",
      "  Train-Val gap: 49.1 hours\n",
      "  Val-Test gap: 74.1 hours\n",
      "  Purged samples: ~576 total (~288 per gap)\n",
      "[PASS] No data leakage detected:\n",
      "  Train-Val gap: 49.1 hours\n",
      "  Val-Test gap: 74.1 hours\n",
      "\n",
      "Dataset sizes:\n",
      "  Train: 808,708\n",
      "  Val:   141,228\n",
      "  Test:  135,996\n",
      "Test batches: 1063\n",
      "Batch size: 128\n",
      "Approx samples: 136064\n"
     ]
    }
   ],
   "source": [
    "from data.dataset import NQDataModule\n",
    "\n",
    "data_module = NQDataModule(\n",
    "    data_path=paths['processed_data'] / 'nq_features_full.parquet',\n",
    "    batch_size=checkpoint['config']['training']['batch_size'],\n",
    "    num_workers=8,\n",
    "    pin_memory=True,\n",
    "    prefetch_factor=2,\n",
    "    persistent_workers=True,\n",
    "    subsample_fraction=1.0,\n",
    "    subsample_seed=42,\n",
    "    apply_subsample_to_all_splits=False\n",
    ")\n",
    "\n",
    "data_module.setup()\n",
    "test_loader = data_module.test_dataloader()\n",
    "\n",
    "print(f\"Test batches: {len(test_loader)}\")\n",
    "print(f\"Batch size: {checkpoint['config']['training']['batch_size']}\")\n",
    "print(f\"Approx samples: {len(test_loader) * checkpoint['config']['training']['batch_size']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tQw3yZ7tTdxN"
   },
   "source": [
    "## 7. Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "orMup0axTdxO",
    "outputId": "337f1ca9-d6f6-40e6-cd99-e236b9a8ddd0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 1063/1063 [08:35<00:00,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predictions shape: (135996, 5, 7)\n",
      "Targets shape: (135996, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from evaluation.inference import ModelPredictor\n",
    "\n",
    "print(\"Running optimized inference with AMP...\")\n",
    "print(\"  Phase 6.1: Async GPU-CPU transfer + FP16 tensor cores\")\n",
    "print(\"  Expected speedup: ~5x vs original implementation\")\n",
    "print()\n",
    "\n",
    "# Initialize optimized predictor\n",
    "# use_amp=True by default, provides 2-3x speedup on A100\n",
    "# Async tensor collection provides additional 2x speedup\n",
    "predictor = ModelPredictor(model, device, use_amp=True)\n",
    "\n",
    "# Run inference\n",
    "result = predictor.predict_dataset(\n",
    "    test_loader, \n",
    "    return_targets=True, \n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "predictions = result['predictions']\n",
    "targets = result['targets']\n",
    "\n",
    "print(f\"\\nPredictions shape: {predictions.shape}\")\n",
    "print(f\"Targets shape: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"INFERENCE PERFORMANCE VALIDATION (Phase 6.1)\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "# Create small test loader for benchmarking\n",
    "test_subset = torch.utils.data.Subset(\n",
    "    test_loader.dataset,\n",
    "    indices=range(min(1024, len(test_loader.dataset)))\n",
    ")\n",
    "test_bench_loader = DataLoader(\n",
    "    test_subset,\n",
    "    batch_size=test_loader.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=test_loader.num_workers,\n",
    "    pin_memory=test_loader.pin_memory\n",
    ")\n",
    "\n",
    "print(f\"Benchmark subset: {len(test_subset)} samples\")\n",
    "print(f\"Batches: {len(test_bench_loader)}\\n\")\n",
    "\n",
    "# Warmup\n",
    "predictor_amp = ModelPredictor(model, device, use_amp=True)\n",
    "_ = predictor_amp.predict_dataset(test_bench_loader, show_progress=False)\n",
    "\n",
    "# Benchmark with AMP\n",
    "start = time.time()\n",
    "result_amp = predictor_amp.predict_dataset(test_bench_loader, show_progress=False)\n",
    "time_amp = time.time() - start\n",
    "\n",
    "# Benchmark without AMP (FP32 only)\n",
    "predictor_fp32 = ModelPredictor(model, device, use_amp=False)\n",
    "start = time.time()\n",
    "result_fp32 = predictor_fp32.predict_dataset(test_bench_loader, show_progress=False)\n",
    "time_fp32 = time.time() - start\n",
    "\n",
    "speedup = time_fp32 / time_amp\n",
    "\n",
    "print(f\"Results:\")\n",
    "print(f\"  FP32 time: {time_fp32:.3f}s\")\n",
    "print(f\"  AMP time:  {time_amp:.3f}s\")\n",
    "print(f\"  Speedup:   {speedup:.2f}x\")\n",
    "print()\n",
    "\n",
    "# Verify accuracy\n",
    "max_error = np.abs(result_amp['predictions'] - result_fp32['predictions']).max()\n",
    "mean_error = np.abs(result_amp['predictions'] - result_fp32['predictions']).mean()\n",
    "\n",
    "print(f\"Numerical accuracy:\")\n",
    "print(f\"  Max relative error:  {max_error:.2e}\")\n",
    "print(f\"  Mean relative error: {mean_error:.2e}\")\n",
    "print(f\"  Status: {'PASS' if max_error < 1e-4 else 'FAIL'} (threshold: 1e-4)\")\n",
    "print()\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fwKhY3KvTdxO"
   },
   "source": [
    "## 8. Compute Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "obk4wOuETdxO",
    "outputId": "7d801350-7b68-4922-a110-faba934b6a92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics...\n",
      "\n",
      "Metrics by horizon:\n",
      "\n",
      "15m:\n",
      "  CRPS: 0.000380\n",
      "  IC: 0.0025\n",
      "  DA: 0.513\n",
      "\n",
      "30m:\n",
      "  CRPS: 0.000528\n",
      "  IC: 0.0051\n",
      "  DA: 0.490\n",
      "\n",
      "60m:\n",
      "  CRPS: 0.000805\n",
      "  IC: 0.0069\n",
      "  DA: 0.467\n",
      "\n",
      "2h:\n",
      "  CRPS: 0.001095\n",
      "  IC: 0.0094\n",
      "  DA: 0.538\n",
      "\n",
      "4h:\n",
      "  CRPS: 0.001604\n",
      "  IC: 0.0060\n",
      "  DA: 0.464\n"
     ]
    }
   ],
   "source": [
    "from evaluation.metrics import MetricsSummary\n",
    "\n",
    "print(\"Computing metrics...\")\n",
    "summary = MetricsSummary(quantiles=quantiles, horizon_names=horizon_names)\n",
    "metrics = summary.compute_all(predictions, targets)\n",
    "\n",
    "print(\"\\nMetrics by horizon:\")\n",
    "for h in horizon_names:\n",
    "    print(f\"\\n{h}:\")\n",
    "    print(f\"  CRPS: {metrics['distributional'][h]['crps']:.6f}\")\n",
    "    print(f\"  IC: {metrics['point'][h]['ic']:.4f}\")\n",
    "    print(f\"  DA: {metrics['point'][h]['da']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LgxMasS-TdxP"
   },
   "source": [
    "## 9. Calibration Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fFxTZr8ZTdxP",
    "outputId": "57bf927b-76ba-4673-c701-095200c7daf4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibration analysis...\n",
      "Saved: calibration_reliability.png\n",
      "\n",
      "Calibration metrics by horizon:\n",
      "  15m:\n",
      "    Mean error: 0.331846\n",
      "    Max error: 0.574276\n",
      "    RMSE: 0.370153\n",
      "  30m:\n",
      "    Mean error: 0.317255\n",
      "    Max error: 0.472508\n",
      "    RMSE: 0.348835\n",
      "  60m:\n",
      "    Mean error: 0.338433\n",
      "    Max error: 0.620218\n",
      "    RMSE: 0.387724\n",
      "  2h:\n",
      "    Mean error: 0.317586\n",
      "    Max error: 0.473626\n",
      "    RMSE: 0.349169\n",
      "  4h:\n",
      "    Mean error: 0.320889\n",
      "    Max error: 0.496825\n",
      "    RMSE: 0.351496\n"
     ]
    }
   ],
   "source": [
    "from evaluation.calibration import CalibrationByHorizon\n",
    "\n",
    "print(\"Calibration analysis...\")\n",
    "cal_analyzer = CalibrationByHorizon(quantiles, horizon_names)\n",
    "calibration_results = cal_analyzer.compute_per_horizon(predictions, targets)\n",
    "\n",
    "fig = cal_analyzer.plot_reliability_by_horizon(predictions, targets)\n",
    "plot_path = paths['results_dir'] / \"calibration_reliability.png\"\n",
    "fig.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "plt.close(fig)\n",
    "print(f\"Saved: {plot_path.name}\")\n",
    "\n",
    "print(\"\\nCalibration metrics by horizon:\")\n",
    "for h in horizon_names:\n",
    "    if h in calibration_results:\n",
    "        print(f\"  {h}:\")\n",
    "        print(f\"    Mean error: {calibration_results[h]['mean_error']:.6f}\")\n",
    "        print(f\"    Max error: {calibration_results[h]['max_error']:.6f}\")\n",
    "        print(f\"    RMSE: {calibration_results[h]['rmse']:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vHCgFZ0ATdxP"
   },
   "source": [
    "## 10. Backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X-kJ1p99TdxP",
    "outputId": "3e03c99a-4898-4cc5-cd9d-34cea60368b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running backtest...\n",
      "\n",
      "Backtest summary:\n",
      "           sharpe   sortino  max_drawdown  profit_factor  hit_rate    calmar  total_return  n_trades  mean_return  std_return\n",
      "horizon                                                                                                                      \n",
      "15m      0.825512  1.141923      0.562152       1.020351  0.513353  0.246666      1.455228    135996     0.000007    0.001256\n",
      "30m     -1.316547 -1.828191      0.946327       0.968808  0.490235 -0.309076     -0.908689    135768    -0.000016    0.001717\n",
      "60m     -1.656696 -2.431951      0.992488       0.961684  0.467234 -0.470706     -0.987165    135996    -0.000029    0.002455\n",
      "2h       2.330175  3.183370      0.998545       1.055629  0.537567  1.832856   1334.804321    135996     0.000059    0.003568\n",
      "4h      -0.836753 -1.234040      0.999992       0.981305  0.464256 -0.564433     -0.996813    135983    -0.000030    0.004995\n",
      "\n",
      "Saved equity curves\n"
     ]
    }
   ],
   "source": [
    "from evaluation.backtest import MultiHorizonBacktester\n",
    "\n",
    "print(\"Running backtest...\")\n",
    "backtester = MultiHorizonBacktester(predictions, targets, horizon_names)\n",
    "bt_results = backtester.run()\n",
    "bt_summary = backtester.get_metrics_summary()\n",
    "\n",
    "print(\"\\nBacktest summary:\")\n",
    "print(bt_summary.to_string())\n",
    "\n",
    "bt_summary.to_csv(paths['results_dir'] / 'backtest_summary.csv')\n",
    "\n",
    "ax = backtester.plot_equity_curves()\n",
    "fig = ax.get_figure()\n",
    "fig.savefig(paths['results_dir'] / 'equity_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.close(fig)\n",
    "print(\"\\nSaved equity curves\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "czqNISsRTdxQ"
   },
   "source": [
    "## 11. Generate Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iIuilOHCTdxQ",
    "outputId": "5f7ed122-eeef-45e2-dad0-9d1fbce8f0ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# MIGT-TVDT Evaluation Report\n",
      "\n",
      "**Date:** 2025-12-09 18:28:23\n",
      "\n",
      "## Training Info\n",
      "- Epochs: 9\n",
      "- Val loss: 0.001176\n",
      "- Subsample: 100%\n",
      "\n",
      "## Architecture\n",
      "- Variables: 24\n",
      "- Max seq len: 288\n",
      "- Horizons: ['15m', '30m', '60m', '2h', '4h']\n",
      "- Quantiles: 7\n",
      "- d_model: 256\n",
      "- Parameters: 6,866,984\n",
      "\n",
      "## Test Set\n",
      "- Samples: 135,996\n",
      "\n",
      "---\n",
      "\n",
      "# Model Evaluation Report\n",
      "\n",
      "Samples evaluated: 135,996\n",
      "\n",
      "\n",
      "## Distributional Metrics\n",
      "\n",
      "| Horizon | CRPS | PICP-80 | PICP-50 | MPIW-80 | MPIW-50 |\n",
      "|---------|------|---------|---------|---------|---------|\n",
      "| 15m | 0.00038 | 0.001 | 0.000 | 0.00000 | 0.00000 |\n",
      "| 30m | 0.00053 | 0.001 | 0.000 | 0.00000 | 0.00000 |\n",
      "| 60m | 0.00081 | 0.000 | 0.000 | 0.00000 | 0.00000 |\n",
      "| 2h | 0.00109 | 0.000 | 0.000 | 0.00000 | 0.00000 |\n",
      "| 4h | 0.00160 | 0.000 | 0.000 | 0.00000 | 0.00000 |\n",
      "\n",
      "## Point Metrics (Median)\n",
      "\n",
      "| Horizon | IC | DA | RMSE | MAE |\n",
      "|---------|----|----|------|-----|\n",
      "| 15m | 0.0025 | 0.513 | 0.00128 | 0.00076 |\n",
      "| 30m | 0.0051 | 0.490 | 0.00178 | 0.00106 |\n",
      "| 60m | 0.0069 | 0.467 | 0.00255 | 0.00161 |\n",
      "| 2h | 0.0094 | 0.538 | 0.00357 | 0.00219 |\n",
      "| 4h | 0.0060 | 0.464 | 0.00507 | 0.00321 |\n",
      "\n",
      "## Calibration Summary\n",
      "\n",
      "- Mean calibration error: 0.3179\n",
      "- Max calibration error: 0.4770\n",
      "## Backtest Results\n",
      "\n",
      "           sharpe   sortino  max_drawdown  profit_factor  hit_rate    calmar  total_return  n_trades  mean_return  std_return\n",
      "horizon                                                                                                                      \n",
      "15m      0.825512  1.141923      0.562152       1.020351  0.513353  0.246666      1.455228    135996     0.000007    0.001256\n",
      "30m     -1.316547 -1.828191      0.946327       0.968808  0.490235 -0.309076     -0.908689    135768    -0.000016    0.001717\n",
      "60m     -1.656696 -2.431951      0.992488       0.961684  0.467234 -0.470706     -0.987165    135996    -0.000029    0.002455\n",
      "2h       2.330175  3.183370      0.998545       1.055629  0.537567  1.832856   1334.804321    135996     0.000059    0.003568\n",
      "4h      -0.836753 -1.234040      0.999992       0.981305  0.464256 -0.564433     -0.996813    135983    -0.000030    0.004995\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from evaluation.inference import format_evaluation_report\n",
    "\n",
    "eval_results = {\n",
    "    'n_samples': len(targets),\n",
    "    'metrics': metrics\n",
    "}\n",
    "\n",
    "report = format_evaluation_report(eval_results, horizon_names)\n",
    "\n",
    "# Defensive extraction: handles both missing 'data' key and None value\n",
    "data_config = checkpoint['config'].get('data', {})\n",
    "subsample_frac = data_config.get('subsample_fraction') or 1.0\n",
    "\n",
    "header = f\"\"\"# MIGT-TVDT Evaluation Report\n",
    "\n",
    "**Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "## Training Info\n",
    "- Epochs: {checkpoint['epoch']}\n",
    "- Val loss: {checkpoint['val_loss']:.6f}\n",
    "- Subsample: {subsample_frac * 100:.0f}%\n",
    "\n",
    "## Architecture\n",
    "- Variables: {model_config['n_variables']}\n",
    "- Max seq len: {model_config['max_seq_len']}\n",
    "- Horizons: {horizon_names}\n",
    "- Quantiles: {len(quantiles)}\n",
    "- d_model: {model_config['d_model']}\n",
    "- Parameters: {n_params:,}\n",
    "\n",
    "## Test Set\n",
    "- Samples: {len(targets):,}\n",
    "\n",
    "---\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Add backtest section\n",
    "backtest_section = \"\\n## Backtest Results\\n\\n\"\n",
    "backtest_section += bt_summary.to_string()\n",
    "backtest_section += \"\\n\"\n",
    "\n",
    "full_report = header + report + backtest_section\n",
    "\n",
    "report_path = paths['results_dir'] / 'evaluation_report.md'\n",
    "with open(report_path, 'w') as f:\n",
    "    f.write(full_report)\n",
    "\n",
    "print(full_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wvWOGOVsTdxQ"
   },
   "source": [
    "## 12. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X20oN2ExTdxR",
    "outputId": "0efd7ca7-848e-4d4e-dc06-37ab22526623"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All results saved to: /content/drive/MyDrive/Colab Notebooks/Transformers/FP/evaluation_results\n"
     ]
    }
   ],
   "source": [
    "# Save predictions\n",
    "np.savez_compressed(\n",
    "    paths['results_dir'] / 'predictions_targets.npz',\n",
    "    predictions=predictions,\n",
    "    targets=targets\n",
    ")\n",
    "\n",
    "# Helper for JSON serialization\n",
    "def to_serializable(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: to_serializable(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, (np.integer, np.floating)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, pd.DataFrame):\n",
    "        return obj.to_dict('records')\n",
    "    return obj\n",
    "\n",
    "# Save metrics\n",
    "with open(paths['results_dir'] / 'metrics.json', 'w') as f:\n",
    "    json.dump(to_serializable(metrics), f, indent=2)\n",
    "\n",
    "# Save calibration\n",
    "with open(paths['results_dir'] / 'calibration.json', 'w') as f:\n",
    "    json.dump(to_serializable(calibration_results), f, indent=2)\n",
    "\n",
    "# Save backtest\n",
    "with open(paths['results_dir'] / 'backtest.json', 'w') as f:\n",
    "    json.dump(to_serializable(bt_results), f, indent=2)\n",
    "\n",
    "# Metrics CSV\n",
    "rows = []\n",
    "for cat in ['distributional', 'point']:\n",
    "    if cat in metrics:\n",
    "        for h, m in metrics[cat].items():\n",
    "            row = {'category': cat, 'horizon': h}\n",
    "            for k, v in m.items():\n",
    "                row[k] = float(v) if isinstance(v, (np.integer, np.floating)) else v\n",
    "            rows.append(row)\n",
    "\n",
    "pd.DataFrame(rows).to_csv(paths['results_dir'] / 'metrics_summary.csv', index=False)\n",
    "\n",
    "print(\"\\nAll results saved to:\", paths['results_dir'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0KchMmLRTdxR"
   },
   "source": [
    "## 13. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V0eFUU7wTdxS",
    "outputId": "f2234a3d-d2f2-439e-b47b-57d4287fb258"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EVALUATION COMPLETE\n",
      "============================================================\n",
      "\n",
      "Key Metrics:\n",
      "\n",
      "15m:\n",
      "  IC: 0.0025\n",
      "  DA: 0.513\n",
      "  CRPS: 0.000380\n",
      "\n",
      "30m:\n",
      "  IC: 0.0051\n",
      "  DA: 0.490\n",
      "  CRPS: 0.000528\n",
      "\n",
      "60m:\n",
      "  IC: 0.0069\n",
      "  DA: 0.467\n",
      "  CRPS: 0.000805\n",
      "\n",
      "2h:\n",
      "  IC: 0.0094\n",
      "  DA: 0.538\n",
      "  CRPS: 0.001095\n",
      "\n",
      "4h:\n",
      "  IC: 0.0060\n",
      "  DA: 0.464\n",
      "  CRPS: 0.001604\n",
      "\n",
      "Backtest Performance:\n",
      "  15m: Sharpe=0.826, Max DD=56.22%\n",
      "  30m: Sharpe=-1.317, Max DD=94.63%\n",
      "  60m: Sharpe=-1.657, Max DD=99.25%\n",
      "  2h: Sharpe=2.330, Max DD=99.85%\n",
      "  4h: Sharpe=-0.837, Max DD=100.00%\n",
      "\n",
      "Results: /content/drive/MyDrive/Colab Notebooks/Transformers/FP/evaluation_results\n",
      "\n",
      "Files:\n",
      "  calibration_reliability.png: 108.3 KB\n",
      "  backtest_summary.csv: 1.0 KB\n",
      "  equity_curves.png: 75.5 KB\n",
      "  evaluation_report.md: 2.1 KB\n",
      "  predictions_targets.npz: 6814.3 KB\n",
      "  metrics.json: 2.4 KB\n",
      "  calibration.json: 1.9 KB\n",
      "  backtest.json: 51150.2 KB\n",
      "  metrics_summary.csv: 1.2 KB\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"EVALUATION COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nKey Metrics:\")\n",
    "for h in horizon_names:\n",
    "    print(f\"\\n{h}:\")\n",
    "    print(f\"  IC: {metrics['point'][h]['ic']:.4f}\")\n",
    "    print(f\"  DA: {metrics['point'][h]['da']:.3f}\")\n",
    "    print(f\"  CRPS: {metrics['distributional'][h]['crps']:.6f}\")\n",
    "\n",
    "print(f\"\\nBacktest Performance:\")\n",
    "for h in horizon_names:\n",
    "    if h in bt_summary.index:\n",
    "        sharpe = bt_summary.loc[h, 'sharpe']\n",
    "        max_dd = bt_summary.loc[h, 'max_drawdown']\n",
    "        print(f\"  {h}: Sharpe={sharpe:.3f}, Max DD={max_dd:.2%}\")\n",
    "\n",
    "print(f\"\\nResults: {paths['results_dir']}\")\n",
    "print(\"\\nFiles:\")\n",
    "for f in paths['results_dir'].glob('*'):\n",
    "    print(f\"  {f.name}: {f.stat().st_size/1024:.1f} KB\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
