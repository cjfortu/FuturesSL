{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULftS6aiTdxA"
      },
      "source": [
        "# Production Model Evaluation\n",
        "\n",
        "Comprehensive evaluation of trained MIGT-TVDT model from Phase 5 outputs.\n",
        "\n",
        "**Evaluation Components:**\n",
        "1. Model loading and inference on full test set\n",
        "2. Distributional metrics (CRPS, calibration, PICP, MPIW)\n",
        "3. Point prediction metrics (IC, DA, RMSE)\n",
        "4. Financial metrics (Sharpe, Sortino, MDD, profit factor)\n",
        "5. Calibration analysis and visualization\n",
        "6. Multi-horizon backtesting\n",
        "7. Comprehensive evaluation report\n",
        "\n",
        "**Outputs:** All results saved to `/content/drive/MyDrive/Colab Notebooks/Transformers/FP/evaluation_results`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IFKxQAwTdxD"
      },
      "source": [
        "## 1. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBE80XeuTdxE",
        "outputId": "400f0cd4-d3c5-4d59-c942-dc5b1e62eead"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Base directory: /content/drive/MyDrive/Colab Notebooks/Transformers/FP\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "BASE_DIR = Path('/content/drive/MyDrive/Colab Notebooks/Transformers/FP')\n",
        "sys.path.insert(0, str(BASE_DIR / 'src'))\n",
        "\n",
        "print(f'Base directory: {BASE_DIR}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iGly_2SHTdxG"
      },
      "outputs": [],
      "source": [
        "!pip install -q scipy matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qASAokHsTdxG",
        "outputId": "b5a983e4-4f67-4cf8-8d88-09389b7c4323"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "GPU: NVIDIA A100-SXM4-80GB\n",
            "VRAM: 85.2 GB\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "from datetime import datetime\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Device: {device}')\n",
        "if torch.cuda.is_available():\n",
        "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
        "    print(f'VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDhgM8_KTdxH"
      },
      "source": [
        "## 2. Configure Paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36vF2kWYTdxH",
        "outputId": "c8a665a1-cfa2-4cfd-f147-243c329b2b79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paths:\n",
            "  checkpoint: exists\n",
            "  training_history: exists\n",
            "  processed_data: exists\n",
            "  results_dir: exists\n"
          ]
        }
      ],
      "source": [
        "paths = {\n",
        "    'checkpoint': BASE_DIR / 'outputs' / 'checkpoint_best.pt',\n",
        "    'training_history': BASE_DIR / 'outputs' / 'training_history.json',\n",
        "    'processed_data': BASE_DIR / 'data' / 'processed',\n",
        "    'results_dir': BASE_DIR / 'evaluation_results'\n",
        "}\n",
        "\n",
        "paths['results_dir'].mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"Paths:\")\n",
        "for key, path in paths.items():\n",
        "    status = \"exists\" if path.exists() else \"missing\"\n",
        "    print(f\"  {key}: {status}\")\n",
        "\n",
        "if not paths['checkpoint'].exists():\n",
        "    raise FileNotFoundError(f\"Checkpoint not found: {paths['checkpoint']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zodpOkuTdxI"
      },
      "source": [
        "## 3. Load Training History"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UgsM6QeTTdxJ",
        "outputId": "8319856c-cca8-4ccf-c865-3881747ef957"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training History:\n",
            "  Epochs: 18\n",
            "  Best epoch: 17\n",
            "  Best val loss: 0.001513\n",
            "  Final train loss: 0.000883\n",
            "  Final val loss: 0.001519\n"
          ]
        }
      ],
      "source": [
        "with open(paths['training_history'], 'r') as f:\n",
        "    history = json.load(f)\n",
        "\n",
        "print(\"Training History:\")\n",
        "print(f\"  Epochs: {len(history['train_loss'])}\")\n",
        "print(f\"  Best epoch: {np.argmin(history['val_loss']) + 1}\")\n",
        "print(f\"  Best val loss: {np.min(history['val_loss']):.6f}\")\n",
        "print(f\"  Final train loss: {history['train_loss'][-1]:.6f}\")\n",
        "print(f\"  Final val loss: {history['val_loss'][-1]:.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPgPnxHBTdxL"
      },
      "source": [
        "## 4. Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHQvt4UlTdxM",
        "outputId": "8f22388f-3309-43ab-805e-22aa30954691"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint: epoch 16, val_loss 0.001513\n",
            "\n",
            "Model config:\n",
            "  n_variables: 24\n",
            "  max_seq_len: 288\n",
            "  n_horizons: 5\n",
            "  n_quantiles: 7\n",
            "  d_model: 256\n",
            "\n",
            "Parameters: 6,866,984\n"
          ]
        }
      ],
      "source": [
        "from model.migt_tvdt import MIGT_TVDT\n",
        "\n",
        "checkpoint = torch.load(paths['checkpoint'], map_location=device)\n",
        "\n",
        "print(f\"Checkpoint: epoch {checkpoint['epoch']}, val_loss {checkpoint['val_loss']:.6f}\")\n",
        "\n",
        "model_config = checkpoint['config']['model']\n",
        "print(f\"\\nModel config:\")\n",
        "print(f\"  n_variables: {model_config['n_variables']}\")\n",
        "print(f\"  max_seq_len: {model_config['max_seq_len']}\")\n",
        "print(f\"  n_horizons: {model_config['n_horizons']}\")\n",
        "print(f\"  n_quantiles: {model_config['n_quantiles']}\")\n",
        "print(f\"  d_model: {model_config['d_model']}\")\n",
        "\n",
        "model = MIGT_TVDT(model_config)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "n_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"\\nParameters: {n_params:,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txW0sWzhTdxM"
      },
      "source": [
        "## 5. Extract Config Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "es2pOpnOTdxN",
        "outputId": "d72128c3-3bcb-4876-f222-ecc7ee0c3392"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quantiles: [0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95]\n",
            "Horizons: ['15m', '30m', '60m', '2h', '4h']\n"
          ]
        }
      ],
      "source": [
        "# Quantiles from checkpoint config\n",
        "if 'quantiles' in checkpoint['config']:\n",
        "    quantiles = checkpoint['config']['quantiles']\n",
        "elif 'quantile_regression' in checkpoint['config']:\n",
        "    quantiles = checkpoint['config']['quantile_regression']['quantiles']\n",
        "else:\n",
        "    # Default from problem statement\n",
        "    quantiles = [0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95]\n",
        "\n",
        "# Horizon names from checkpoint or default\n",
        "if 'horizon_names' in checkpoint['config']:\n",
        "    horizon_names = checkpoint['config']['horizon_names']\n",
        "elif 'horizons' in checkpoint['config'].get('model', {}):\n",
        "    horizon_names = checkpoint['config']['model']['horizons']\n",
        "else:\n",
        "    # Default from problem statement\n",
        "    horizon_names = ['15m', '30m', '60m', '2h', '4h']\n",
        "\n",
        "print(f\"Quantiles: {quantiles}\")\n",
        "print(f\"Horizons: {horizon_names}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1kj9VwUTdxN"
      },
      "source": [
        "## 6. Load Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kh_D3AmqTdxN",
        "outputId": "ce26ca51-01e2-4d77-87d8-cd714309ddce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data from /content/drive/MyDrive/Colab Notebooks/Transformers/FP/data/processed/nq_features_full.parquet\n",
            "Features: 24\n",
            "Targets: 5\n",
            "Split statistics:\n",
            "  Train: 808,996 samples (2010-06-07 to 2021-12-31)\n",
            "  Val:   141,516 samples (2022-01-02 to 2023-12-29)\n",
            "  Test:  136,284 samples (2024-01-02 to 2025-12-03)\n",
            "\n",
            "Temporal gaps:\n",
            "  Train-Val gap: 49.1 hours\n",
            "  Val-Test gap: 74.1 hours\n",
            "  Purged samples: ~576 total (~288 per gap)\n",
            "[PASS] No data leakage detected:\n",
            "  Train-Val gap: 49.1 hours\n",
            "  Val-Test gap: 74.1 hours\n",
            "\n",
            "Dataset sizes:\n",
            "  Train: 808,708\n",
            "  Val:   141,228\n",
            "  Test:  135,996\n",
            "Test batches: 1104\n",
            "Batch size: 128\n",
            "Approx samples: 141312\n"
          ]
        }
      ],
      "source": [
        "from data.dataset import NQDataModule\n",
        "\n",
        "data_module = NQDataModule(\n",
        "    data_path=paths['processed_data'] / 'nq_features_full.parquet',\n",
        "    batch_size=checkpoint['config']['training']['batch_size'],\n",
        "    num_workers=8,\n",
        "    pin_memory=True,\n",
        "    prefetch_factor=2,\n",
        "    persistent_workers=True,\n",
        "    subsample_fraction=1.0,\n",
        "    subsample_seed=42,\n",
        "    apply_subsample_to_all_splits=False\n",
        ")\n",
        "\n",
        "data_module.setup()\n",
        "test_loader = data_module.val_dataloader()\n",
        "\n",
        "print(f\"Test batches: {len(test_loader)}\")\n",
        "print(f\"Batch size: {checkpoint['config']['training']['batch_size']}\")\n",
        "print(f\"Approx samples: {len(test_loader) * checkpoint['config']['training']['batch_size']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQw3yZ7tTdxN"
      },
      "source": [
        "## 7. Run Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "orMup0axTdxO",
        "outputId": "5958c9fc-6a40-4091-cfbb-4babbbf349d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running optimized inference with AMP...\n",
            "  Phase 6.1: Async GPU-CPU transfer + FP16 tensor cores\n",
            "  Expected speedup: ~5x vs original implementation\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rPredicting:   0%|          | 0/1104 [00:00<?, ?it/s]/content/drive/MyDrive/Colab Notebooks/Transformers/FP/src/evaluation/inference.py:108: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=self.use_amp):\n",
            "Predicting: 100%|██████████| 1104/1104 [02:40<00:00,  6.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Predictions shape: (141228, 5, 7)\n",
            "Targets shape: (141228, 5)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from evaluation.inference import ModelPredictor\n",
        "\n",
        "print(\"Running optimized inference with AMP...\")\n",
        "print(\"  Phase 6.1: Async GPU-CPU transfer + FP16 tensor cores\")\n",
        "print(\"  Expected speedup: ~5x vs original implementation\")\n",
        "print()\n",
        "\n",
        "# Initialize optimized predictor\n",
        "# use_amp=True by default, provides 2-3x speedup on A100\n",
        "# Async tensor collection provides additional 2x speedup\n",
        "predictor = ModelPredictor(model, device, use_amp=True)\n",
        "\n",
        "# Run inference\n",
        "result = predictor.predict_dataset(\n",
        "    test_loader,\n",
        "    return_targets=True,\n",
        "    show_progress=True\n",
        ")\n",
        "\n",
        "predictions = result['predictions']\n",
        "targets = result['targets']\n",
        "\n",
        "print(f\"\\nPredictions shape: {predictions.shape}\")\n",
        "print(f\"Targets shape: {targets.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OThEpf-HSDtY",
        "outputId": "9f742b54-c95d-4c94-a9c5-b8a0238f0bb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "INFERENCE PERFORMANCE VALIDATION (Phase 6.1)\n",
            "============================================================\n",
            "\n",
            "Benchmarking: First 8 batches from test set\n",
            "Approx samples: 1024\n",
            "\n",
            "Performance Results:\n",
            "  FP32 time: 4.689s (0.586s/batch)\n",
            "  AMP time:  2.110s (0.264s/batch)\n",
            "  Speedup:   2.22x\n",
            "\n",
            "Numerical Accuracy:\n",
            "  Max absolute error: 2.86e-04\n",
            "  Mean absolute error: 6.75e-05\n",
            "  Max relative error: 6.47e-01\n",
            "  Status: ✗ FAIL (threshold: 1e-4)\n",
            "\n",
            "Performance Assessment:\n",
            "  Expected on A100: 2.5-3.0x speedup from AMP\n",
            "  Your result: 2.22x - ✓ GOOD\n",
            "\n",
            "Note: Combined with async fix (~2x), total speedup vs original ~5x\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"INFERENCE PERFORMANCE VALIDATION (Phase 6.1)\")\n",
        "print(\"=\" * 60)\n",
        "print()\n",
        "\n",
        "# Benchmark on subset by limiting number of batches (not using Subset)\n",
        "n_batches = min(8, len(test_loader))\n",
        "print(f\"Benchmarking: First {n_batches} batches from test set\")\n",
        "print(f\"Approx samples: {n_batches * test_loader.batch_size}\\n\")\n",
        "\n",
        "# Helper function\n",
        "def benchmark_inference(predictor, loader, n_batches):\n",
        "    \"\"\"Run inference on first n_batches and return time + predictions.\"\"\"\n",
        "    start = time.time()\n",
        "    preds = []\n",
        "    count = 0\n",
        "    for batch in loader:\n",
        "        outputs = predictor.predict_batch(batch)\n",
        "        preds.append(outputs['quantiles'].cpu())\n",
        "        count += 1\n",
        "        if count >= n_batches:\n",
        "            break\n",
        "    elapsed = time.time() - start\n",
        "    return elapsed, torch.cat(preds, dim=0).numpy()\n",
        "\n",
        "# Warmup GPU\n",
        "predictor_amp = ModelPredictor(model, device, use_amp=True)\n",
        "_, _ = benchmark_inference(predictor_amp, test_loader, 2)\n",
        "\n",
        "# Benchmark with AMP (FP16)\n",
        "time_amp, preds_amp = benchmark_inference(predictor_amp, test_loader, n_batches)\n",
        "\n",
        "# Benchmark without AMP (FP32)\n",
        "predictor_fp32 = ModelPredictor(model, device, use_amp=False)\n",
        "time_fp32, preds_fp32 = benchmark_inference(predictor_fp32, test_loader, n_batches)\n",
        "\n",
        "# Calculate speedup\n",
        "speedup = time_fp32 / time_amp\n",
        "\n",
        "print(f\"Performance Results:\")\n",
        "print(f\"  FP32 time: {time_fp32:.3f}s ({time_fp32/n_batches:.3f}s/batch)\")\n",
        "print(f\"  AMP time:  {time_amp:.3f}s ({time_amp/n_batches:.3f}s/batch)\")\n",
        "print(f\"  Speedup:   {speedup:.2f}x\")\n",
        "print()\n",
        "\n",
        "# Verify numerical accuracy\n",
        "max_error = np.abs(preds_amp - preds_fp32).max()\n",
        "mean_error = np.abs(preds_amp - preds_fp32).mean()\n",
        "rel_error = max_error / (np.abs(preds_fp32).mean() + 1e-8)\n",
        "\n",
        "print(f\"Numerical Accuracy:\")\n",
        "print(f\"  Max absolute error: {max_error:.2e}\")\n",
        "print(f\"  Mean absolute error: {mean_error:.2e}\")\n",
        "print(f\"  Max relative error: {rel_error:.2e}\")\n",
        "print(f\"  Status: {'✓ PASS' if rel_error < 1e-4 else '✗ FAIL'} (threshold: 1e-4)\")\n",
        "print()\n",
        "\n",
        "# Performance assessment\n",
        "print(f\"Performance Assessment:\")\n",
        "print(f\"  Expected on A100: 2.5-3.0x speedup from AMP\")\n",
        "print(f\"  Your result: {speedup:.2f}x - {'✓ EXCELLENT' if speedup > 2.5 else '✓ GOOD' if speedup > 2.0 else '⚠ CHECK GPU TYPE'}\")\n",
        "print()\n",
        "print(\"Note: Combined with async fix (~2x), total speedup vs original ~5x\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwKhY3KvTdxO"
      },
      "source": [
        "## 8. Compute Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obk4wOuETdxO",
        "outputId": "f6ab8fdc-a994-4126-bea8-63459e974a06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing metrics...\n",
            "\n",
            "Metrics by horizon:\n",
            "\n",
            "15m:\n",
            "  CRPS: 0.000500\n",
            "  IC: 0.0026\n",
            "  DA: 0.487\n",
            "\n",
            "30m:\n",
            "  CRPS: 0.000770\n",
            "  IC: -0.0014\n",
            "  DA: 0.485\n",
            "\n",
            "60m:\n",
            "  CRPS: 0.001034\n",
            "  IC: 0.0027\n",
            "  DA: 0.511\n",
            "\n",
            "2h:\n",
            "  CRPS: 0.001503\n",
            "  IC: 0.0220\n",
            "  DA: 0.516\n",
            "\n",
            "4h:\n",
            "  CRPS: 0.002158\n",
            "  IC: 0.0220\n",
            "  DA: 0.517\n"
          ]
        }
      ],
      "source": [
        "from evaluation.metrics import MetricsSummary\n",
        "\n",
        "print(\"Computing metrics...\")\n",
        "summary = MetricsSummary(quantiles=quantiles, horizon_names=horizon_names)\n",
        "metrics = summary.compute_all(predictions, targets)\n",
        "\n",
        "print(\"\\nMetrics by horizon:\")\n",
        "for h in horizon_names:\n",
        "    print(f\"\\n{h}:\")\n",
        "    print(f\"  CRPS: {metrics['distributional'][h]['crps']:.6f}\")\n",
        "    print(f\"  IC: {metrics['point'][h]['ic']:.4f}\")\n",
        "    print(f\"  DA: {metrics['point'][h]['da']:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgxMasS-TdxP"
      },
      "source": [
        "## 9. Calibration Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFxTZr8ZTdxP",
        "outputId": "f8a36182-1d85-4a6d-ce64-e1724fe786c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calibration analysis...\n",
            "Saved: calibration_reliability.png\n",
            "\n",
            "Calibration metrics by horizon:\n",
            "  15m:\n",
            "    Mean error: 0.323575\n",
            "    Max error: 0.517536\n",
            "    RMSE: 0.354565\n",
            "  30m:\n",
            "    Mean error: 0.340619\n",
            "    Max error: 0.634553\n",
            "    RMSE: 0.394315\n",
            "  60m:\n",
            "    Mean error: 0.325496\n",
            "    Max error: 0.528547\n",
            "    RMSE: 0.357197\n",
            "  2h:\n",
            "    Mean error: 0.329751\n",
            "    Max error: 0.558328\n",
            "    RMSE: 0.364909\n",
            "  4h:\n",
            "    Mean error: 0.323236\n",
            "    Max error: 0.512806\n",
            "    RMSE: 0.354058\n"
          ]
        }
      ],
      "source": [
        "from evaluation.calibration import CalibrationByHorizon\n",
        "\n",
        "print(\"Calibration analysis...\")\n",
        "cal_analyzer = CalibrationByHorizon(quantiles, horizon_names)\n",
        "calibration_results = cal_analyzer.compute_per_horizon(predictions, targets)\n",
        "\n",
        "fig = cal_analyzer.plot_reliability_by_horizon(predictions, targets)\n",
        "plot_path = paths['results_dir'] / \"calibration_reliability.png\"\n",
        "fig.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
        "plt.close(fig)\n",
        "print(f\"Saved: {plot_path.name}\")\n",
        "\n",
        "print(\"\\nCalibration metrics by horizon:\")\n",
        "for h in horizon_names:\n",
        "    if h in calibration_results:\n",
        "        print(f\"  {h}:\")\n",
        "        print(f\"    Mean error: {calibration_results[h]['mean_error']:.6f}\")\n",
        "        print(f\"    Max error: {calibration_results[h]['max_error']:.6f}\")\n",
        "        print(f\"    RMSE: {calibration_results[h]['rmse']:.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHCgFZ0ATdxP"
      },
      "source": [
        "## 10. Backtest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-kJ1p99TdxP",
        "outputId": "e3c3dfc9-39ac-4797-c71c-16a3291684ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running backtest...\n",
            "\n",
            "Backtest summary:\n",
            "           sharpe   sortino  max_drawdown  profit_factor  hit_rate    calmar  total_return  n_trades  mean_return  std_return\n",
            "horizon                                                                                                                      \n",
            "15m     -0.212200 -0.301717      0.692361       0.995109  0.486582 -0.084733     -0.352378    141228    -0.000002    0.001392\n",
            "30m     -0.388238 -0.554579      0.916353       0.991186  0.485088 -0.154195     -0.665294    141228    -0.000006    0.002044\n",
            "60m      0.572024  0.802242      0.970339       1.012977  0.510579  0.174382      2.075193    141228     0.000013    0.003222\n",
            "2h       0.822033  1.152040      0.995815       1.018401  0.516017  0.338176      7.049192    141228     0.000021    0.003663\n",
            "4h       1.081520  1.511463      0.999999       1.023525  0.517079  0.766481     58.695076    141228     0.000050    0.006459\n",
            "\n",
            "Saved equity curves\n"
          ]
        }
      ],
      "source": [
        "from evaluation.backtest import MultiHorizonBacktester\n",
        "\n",
        "print(\"Running backtest...\")\n",
        "backtester = MultiHorizonBacktester(predictions, targets, horizon_names)\n",
        "bt_results = backtester.run()\n",
        "bt_summary = backtester.get_metrics_summary()\n",
        "\n",
        "print(\"\\nBacktest summary:\")\n",
        "print(bt_summary.to_string())\n",
        "\n",
        "bt_summary.to_csv(paths['results_dir'] / 'backtest_summary.csv')\n",
        "\n",
        "ax = backtester.plot_equity_curves()\n",
        "fig = ax.get_figure()\n",
        "fig.savefig(paths['results_dir'] / 'equity_curves.png', dpi=150, bbox_inches='tight')\n",
        "plt.close(fig)\n",
        "print(\"\\nSaved equity curves\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czqNISsRTdxQ"
      },
      "source": [
        "## 11. Generate Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iIuilOHCTdxQ",
        "outputId": "2d6925c5-5433-4fe6-877a-0811d33b08d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# MIGT-TVDT Evaluation Report\n",
            "\n",
            "**Date:** 2025-12-12 16:38:35\n",
            "\n",
            "## Training Info\n",
            "- Epochs: 16\n",
            "- Val loss: 0.001513\n",
            "- Subsample: 100%\n",
            "\n",
            "## Architecture\n",
            "- Variables: 24\n",
            "- Max seq len: 288\n",
            "- Horizons: ['15m', '30m', '60m', '2h', '4h']\n",
            "- Quantiles: 7\n",
            "- d_model: 256\n",
            "- Parameters: 6,866,984\n",
            "\n",
            "## Test Set\n",
            "- Samples: 141,228\n",
            "\n",
            "---\n",
            "\n",
            "# Model Evaluation Report\n",
            "\n",
            "Samples evaluated: 141,228\n",
            "\n",
            "\n",
            "## Distributional Metrics\n",
            "\n",
            "| Horizon | CRPS | PICP-80 | PICP-50 | MPIW-80 | MPIW-50 |\n",
            "|---------|------|---------|---------|---------|---------|\n",
            "| 15m | 0.00050 | 0.000 | 0.000 | 0.00000 | 0.00000 |\n",
            "| 30m | 0.00077 | 0.000 | 0.000 | 0.00000 | 0.00000 |\n",
            "| 60m | 0.00103 | 0.000 | 0.000 | 0.00000 | 0.00000 |\n",
            "| 2h | 0.00150 | 0.000 | 0.000 | 0.00000 | 0.00000 |\n",
            "| 4h | 0.00216 | 0.000 | 0.000 | 0.00000 | 0.00000 |\n",
            "\n",
            "## Point Metrics (Median)\n",
            "\n",
            "| Horizon | IC | DA | RMSE | MAE |\n",
            "|---------|----|----|------|-----|\n",
            "| 15m | 0.0026 | 0.487 | 0.00161 | 0.00100 |\n",
            "| 30m | -0.0014 | 0.485 | 0.00234 | 0.00154 |\n",
            "| 60m | 0.0027 | 0.511 | 0.00324 | 0.00207 |\n",
            "| 2h | 0.0220 | 0.516 | 0.00462 | 0.00301 |\n",
            "| 4h | 0.0220 | 0.517 | 0.00649 | 0.00432 |\n",
            "\n",
            "## Calibration Summary\n",
            "\n",
            "- Mean calibration error: 0.3143\n",
            "- Max calibration error: 0.4504\n",
            "## Backtest Results\n",
            "\n",
            "           sharpe   sortino  max_drawdown  profit_factor  hit_rate    calmar  total_return  n_trades  mean_return  std_return\n",
            "horizon                                                                                                                      \n",
            "15m     -0.212200 -0.301717      0.692361       0.995109  0.486582 -0.084733     -0.352378    141228    -0.000002    0.001392\n",
            "30m     -0.388238 -0.554579      0.916353       0.991186  0.485088 -0.154195     -0.665294    141228    -0.000006    0.002044\n",
            "60m      0.572024  0.802242      0.970339       1.012977  0.510579  0.174382      2.075193    141228     0.000013    0.003222\n",
            "2h       0.822033  1.152040      0.995815       1.018401  0.516017  0.338176      7.049192    141228     0.000021    0.003663\n",
            "4h       1.081520  1.511463      0.999999       1.023525  0.517079  0.766481     58.695076    141228     0.000050    0.006459\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from evaluation.inference import format_evaluation_report\n",
        "\n",
        "eval_results = {\n",
        "    'n_samples': len(targets),\n",
        "    'metrics': metrics\n",
        "}\n",
        "\n",
        "report = format_evaluation_report(eval_results, horizon_names)\n",
        "\n",
        "# Defensive extraction: handles both missing 'data' key and None value\n",
        "data_config = checkpoint['config'].get('data', {})\n",
        "subsample_frac = data_config.get('subsample_fraction') or 1.0\n",
        "\n",
        "header = f\"\"\"# MIGT-TVDT Evaluation Report\n",
        "\n",
        "**Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "\n",
        "## Training Info\n",
        "- Epochs: {checkpoint['epoch']}\n",
        "- Val loss: {checkpoint['val_loss']:.6f}\n",
        "- Subsample: {subsample_frac * 100:.0f}%\n",
        "\n",
        "## Architecture\n",
        "- Variables: {model_config['n_variables']}\n",
        "- Max seq len: {model_config['max_seq_len']}\n",
        "- Horizons: {horizon_names}\n",
        "- Quantiles: {len(quantiles)}\n",
        "- d_model: {model_config['d_model']}\n",
        "- Parameters: {n_params:,}\n",
        "\n",
        "## Test Set\n",
        "- Samples: {len(targets):,}\n",
        "\n",
        "---\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Add backtest section\n",
        "backtest_section = \"\\n## Backtest Results\\n\\n\"\n",
        "backtest_section += bt_summary.to_string()\n",
        "backtest_section += \"\\n\"\n",
        "\n",
        "full_report = header + report + backtest_section\n",
        "\n",
        "report_path = paths['results_dir'] / 'evaluation_report.md'\n",
        "with open(report_path, 'w') as f:\n",
        "    f.write(full_report)\n",
        "\n",
        "print(full_report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvWOGOVsTdxQ"
      },
      "source": [
        "## 12. Save Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X20oN2ExTdxR",
        "outputId": "5644c254-3533-489e-d38e-8d4b07fae00a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "All results saved to: /content/drive/MyDrive/Colab Notebooks/Transformers/FP/evaluation_results\n"
          ]
        }
      ],
      "source": [
        "# Save predictions\n",
        "np.savez_compressed(\n",
        "    paths['results_dir'] / 'predictions_targets.npz',\n",
        "    predictions=predictions,\n",
        "    targets=targets\n",
        ")\n",
        "\n",
        "# Helper for JSON serialization\n",
        "def to_serializable(obj):\n",
        "    if isinstance(obj, dict):\n",
        "        return {k: to_serializable(v) for k, v in obj.items()}\n",
        "    elif isinstance(obj, (np.integer, np.floating)):\n",
        "        return float(obj)\n",
        "    elif isinstance(obj, np.ndarray):\n",
        "        return obj.tolist()\n",
        "    elif isinstance(obj, pd.DataFrame):\n",
        "        return obj.to_dict('records')\n",
        "    return obj\n",
        "\n",
        "# Save metrics\n",
        "with open(paths['results_dir'] / 'metrics.json', 'w') as f:\n",
        "    json.dump(to_serializable(metrics), f, indent=2)\n",
        "\n",
        "# Save calibration\n",
        "with open(paths['results_dir'] / 'calibration.json', 'w') as f:\n",
        "    json.dump(to_serializable(calibration_results), f, indent=2)\n",
        "\n",
        "# Save backtest\n",
        "with open(paths['results_dir'] / 'backtest.json', 'w') as f:\n",
        "    json.dump(to_serializable(bt_results), f, indent=2)\n",
        "\n",
        "# Metrics CSV\n",
        "rows = []\n",
        "for cat in ['distributional', 'point']:\n",
        "    if cat in metrics:\n",
        "        for h, m in metrics[cat].items():\n",
        "            row = {'category': cat, 'horizon': h}\n",
        "            for k, v in m.items():\n",
        "                row[k] = float(v) if isinstance(v, (np.integer, np.floating)) else v\n",
        "            rows.append(row)\n",
        "\n",
        "pd.DataFrame(rows).to_csv(paths['results_dir'] / 'metrics_summary.csv', index=False)\n",
        "\n",
        "print(\"\\nAll results saved to:\", paths['results_dir'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KchMmLRTdxR"
      },
      "source": [
        "## 13. Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0eFUU7wTdxS",
        "outputId": "dcd4929a-0beb-46e1-f367-07adb67c317c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "EVALUATION COMPLETE\n",
            "============================================================\n",
            "\n",
            "Key Metrics:\n",
            "\n",
            "15m:\n",
            "  IC: 0.0026\n",
            "  DA: 0.487\n",
            "  CRPS: 0.000500\n",
            "\n",
            "30m:\n",
            "  IC: -0.0014\n",
            "  DA: 0.485\n",
            "  CRPS: 0.000770\n",
            "\n",
            "60m:\n",
            "  IC: 0.0027\n",
            "  DA: 0.511\n",
            "  CRPS: 0.001034\n",
            "\n",
            "2h:\n",
            "  IC: 0.0220\n",
            "  DA: 0.516\n",
            "  CRPS: 0.001503\n",
            "\n",
            "4h:\n",
            "  IC: 0.0220\n",
            "  DA: 0.517\n",
            "  CRPS: 0.002158\n",
            "\n",
            "Backtest Performance:\n",
            "  15m: Sharpe=-0.212, Max DD=69.24%\n",
            "  30m: Sharpe=-0.388, Max DD=91.64%\n",
            "  60m: Sharpe=0.572, Max DD=97.03%\n",
            "  2h: Sharpe=0.822, Max DD=99.58%\n",
            "  4h: Sharpe=1.082, Max DD=100.00%\n",
            "\n",
            "Results: /content/drive/MyDrive/Colab Notebooks/Transformers/FP/evaluation_results\n",
            "\n",
            "Files:\n",
            "  calibration_reliability.png: 110.3 KB\n",
            "  backtest_summary.csv: 1.0 KB\n",
            "  equity_curves.png: 68.5 KB\n",
            "  calibration.json: 1.9 KB\n",
            "  evaluation_report.md: 2.1 KB\n",
            "  metrics.json: 2.4 KB\n",
            "  predictions_targets.npz: 2822.9 KB\n",
            "  backtest.json: 52420.7 KB\n",
            "  metrics_summary.csv: 1.2 KB\n"
          ]
        }
      ],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"EVALUATION COMPLETE\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\nKey Metrics:\")\n",
        "for h in horizon_names:\n",
        "    print(f\"\\n{h}:\")\n",
        "    print(f\"  IC: {metrics['point'][h]['ic']:.4f}\")\n",
        "    print(f\"  DA: {metrics['point'][h]['da']:.3f}\")\n",
        "    print(f\"  CRPS: {metrics['distributional'][h]['crps']:.6f}\")\n",
        "\n",
        "print(f\"\\nBacktest Performance:\")\n",
        "for h in horizon_names:\n",
        "    if h in bt_summary.index:\n",
        "        sharpe = bt_summary.loc[h, 'sharpe']\n",
        "        max_dd = bt_summary.loc[h, 'max_drawdown']\n",
        "        print(f\"  {h}: Sharpe={sharpe:.3f}, Max DD={max_dd:.2%}\")\n",
        "\n",
        "print(f\"\\nResults: {paths['results_dir']}\")\n",
        "print(\"\\nFiles:\")\n",
        "for f in paths['results_dir'].glob('*'):\n",
        "    print(f\"  {f.name}: {f.stat().st_size/1024:.1f} KB\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python [conda env:base] *",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}